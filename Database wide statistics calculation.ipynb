{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate stastistics for an entire database and sectors within the database\n",
    "\n",
    "Work with Python 3.6 and HDF5 storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step : calculating indicators for each {activity|impact method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "#MC_results_dict={act:{ic_name:[MC_results]}} as the output of MC_multi_impact_entire_DB()\n",
    "\n",
    "#Stored MC results in HDF5 are np array 1d which size=# iteration\n",
    "#and stored like: Uncertainty LCI 1 LCIA 1/ActKey/impact method name\n",
    "\n",
    "\n",
    "\n",
    "def gini_coefficient(src):\n",
    "    out = []\n",
    "    for i in range(0,len(src)):\n",
    "        for j in range(i+1,len(src)):\n",
    "            out.append(abs(src[i]-src[j]))\n",
    "    avdiff = np.mean(out)\n",
    "    mn = np.mean(src)\n",
    "    return avdiff / (2*mn);\n",
    "\n",
    "\n",
    "def calculating_endpoint_sum(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics):\n",
    "    \n",
    "    #Calculating sum\n",
    "    for uncertainty_level in hdf5_file_MC_LCA_results.items():\n",
    "        \n",
    "        if 'lci_iteration_name_list' not in uncertainty_level[0]:\n",
    "\n",
    "            for act in uncertainty_level[1].items():                    \n",
    "                \n",
    "                for impact_method in act[1].items():\n",
    "                                        \n",
    "                    #If endpoint names are the second name in impact method tuples (...,...,...)\n",
    "                    endpoint_name='{},{})'.format(impact_method[0].split(',', 3)[0],impact_method[0].split(',', 3)[1])\n",
    "\n",
    "                    #If endpoint names are the first name in impact method tuples (...,...,...)\n",
    "                    #endpoint_name=impact_method[0].rsplit(',', 2)[0]+')'\n",
    "\n",
    "                    endpoint_group_path='/{}/{}/{}'.format(uncertainty_level[0],act[0],endpoint_name)\n",
    "\n",
    "                    contribution_to_add=impact_method[1][()]\n",
    "\n",
    "                    try:\n",
    "                        endpoint_sum_dataset=hdf5_file_MC_statistics['{}/endpoint_sum'.format(endpoint_group_path)]\n",
    "                        endpoint_sum_dataset[...]=endpoint_sum_dataset[()]+contribution_to_add\n",
    "\n",
    "                    except:\n",
    "                        hdf5_file_MC_statistics.create_dataset('{}/endpoint_sum'.format(endpoint_group_path),data=contribution_to_add)\n",
    "\n",
    "    #Calculating variance\n",
    "    for uncertainty_level in hdf5_file_MC_statistics.items():\n",
    "\n",
    "        for act in uncertainty_level[1].items():                    \n",
    "\n",
    "            for impact_method in act[1].items():\n",
    "\n",
    "                impact_method_group_path='/{}/{}/{}'.format(uncertainty_level[0],act[0],impact_method[0])\n",
    "                \n",
    "                endpoint_sum=hdf5_file_MC_statistics['{}/endpoint_sum'.format(impact_method_group_path)]\n",
    "                data=np.var(endpoint_sum)\n",
    "                hdf5_file_MC_statistics.create_dataset('{}/variance'.format(impact_method_group_path),data=data)\n",
    "         \n",
    "    return;\n",
    "\n",
    "\n",
    "\n",
    "def sensivity_index_1st_estimate_smooth_curve(Y,X,bin_size=50):\n",
    "    \n",
    "    #Gathering Y and X\n",
    "    pairs=np.column_stack((Y,X))\n",
    "    \n",
    "    \n",
    "    #Sorting by X ascending\n",
    "    pairs=pairs[pairs[:, 1].argsort()]\n",
    "    \n",
    "    \n",
    "    #Number of bins\n",
    "    if Y.size%bin_size != 0:\n",
    "        print(\"bin_size should be adjusted to be a multiple of Y size\")\n",
    "        return;\n",
    "    \n",
    "    if bin_size%int(bin_size) != 0:\n",
    "        print(\"bin_size should be an integer\")\n",
    "        return;\n",
    "    \n",
    "    bins=int(Y.size/bin_size)\n",
    "    bin_size=int(bin_size)\n",
    "    \n",
    "    \n",
    "    #Calculating mean for each bin\n",
    "    data=pairs[:,0]\n",
    "    data=np.reshape(data,(bins,bin_size))\n",
    "    bin_means=np.mean(data, axis=1)\n",
    "    \n",
    "    \n",
    "    #sensivity_index_1st\n",
    "    si_1st=np.var(bin_means)/np.var(Y)\n",
    "    \n",
    "    return si_1st;\n",
    "\n",
    "\n",
    "# for independant parameters model, to be used for sensitivity between mutated models and IC ranking\n",
    "def sensivity_index_1st_variance_ratio(var_Y,var_X): \n",
    "    \n",
    "    try:\n",
    "        si_1st=var_X/var_Y\n",
    "    except ZeroDivisionError:\n",
    "        si_1st=0\n",
    "        \n",
    "    return si_1st;\n",
    "\n",
    "\n",
    "\n",
    "#MC_results_dict={act_key:{ic_name:[MC_results]}} as the output of MC_multi_impact_entire_DB()\n",
    "\n",
    "def calculating_endpoint_stats_indicators(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics,bin_size,regular_stats=1, dispersion_stats=1, sensitivity_stats=1):\n",
    "  \n",
    "    sum_spear_corr_endpoint={}\n",
    "    \n",
    "    endpoint_name_list=[]\n",
    "    \n",
    "    for uncertainty_level in hdf5_file_MC_LCA_results.items():\n",
    "\n",
    "        if 'lci_iteration_name_list' not in uncertainty_level[0]:\n",
    "\n",
    "            for act in uncertainty_level[1].items():                    \n",
    "\n",
    "                for impact_method in act[1].items():\n",
    "                    \n",
    "                    #If endpoint names are the second name in impact method tuples (...,...,...)\n",
    "                    endpoint_name='{},{})'.format(impact_method[0].split(',', 3)[0],impact_method[0].split(',', 3)[1])\n",
    "\n",
    "\n",
    "                    #If endpoint names are the first name in impact method tuples (...,...,...)\n",
    "                    #endpoint_name=impact_method[0].rsplit(',', 2)[0]+')'\n",
    "\n",
    "                    endpoint_group_path='/{}/{}/{}'.format(uncertainty_level[0],act[0],endpoint_name)\n",
    "\n",
    "                    stats_dict={}\n",
    "\n",
    "                    #Regular stats\n",
    "                    if regular_stats==1:\n",
    "                        stats_dict['mean']=np.mean(impact_method[1])\n",
    "                        stats_dict['variance']=np.var(impact_method[1])\n",
    "                        stats_dict['std dev']=np.std(impact_method[1])\n",
    "                        stats_dict['minimum']=min(impact_method[1])\n",
    "                        stats_dict['maximum']=max(impact_method[1])\n",
    "                        stats_dict['2.5th percentile']=np.percentile(impact_method[1],2.5)\n",
    "                        stats_dict['25th percentile']=np.percentile(impact_method[1],25)\n",
    "                        stats_dict['median']=np.percentile(impact_method[1],50)\n",
    "                        stats_dict['75th percentile']=np.percentile(impact_method[1],75)\n",
    "                        stats_dict['97.5th percentile']=np.percentile(impact_method[1],97.5)\n",
    "                        stats_dict['number of iterations']=len(impact_method[1])\n",
    "\n",
    "                    #Stats to measure the dispersion\n",
    "\n",
    "                    if dispersion_stats==1:\n",
    "                        stats_dict['MADM']=np.percentile(abs(impact_method[1]-stats_dict['median']),50)\n",
    "                        stats_dict['IQR']=stats_dict['75th percentile']-stats_dict['25th percentile']\n",
    "                        stats_dict['Spread']=stats_dict['maximum']-stats_dict['minimum']\n",
    "                        stats_dict['CI95']=stats_dict['97.5th percentile']-stats_dict['2.5th percentile']\n",
    "                        try:\n",
    "                            stats_dict['Quartile coeff of dispersion']=stats_dict['IQR']/(stats_dict['75th percentile']+stats_dict['25th percentile'])\n",
    "                        except ZeroDivisionError:\n",
    "                            stats_dict['Quartile coeff of dispersion']='NA'\n",
    "                        try:\n",
    "                            stats_dict['CV']=stats_dict['std dev']/stats_dict['mean']\n",
    "                        except ZeroDivisionError:\n",
    "                            stats_dict['CV']='NA'\n",
    "                        try:\n",
    "                            stats_dict['CV modified']=stats_dict['std dev']/np.sqrt((stats_dict['maximum']-stats_dict['mean'])*(stats_dict['mean']-stats_dict['minimum']))\n",
    "                        except ZeroDivisionError:\n",
    "                            stats_dict['CV modified']='NA'\n",
    "                        try:\n",
    "                            stats_dict['CV robust']=stats_dict['MADM']/stats_dict['median']\n",
    "                        except ZeroDivisionError:\n",
    "                            stats_dict['CV robust']='NA'\n",
    "                        try:\n",
    "                            stats_dict['IQR\\spread']=stats_dict['IQR']/(stats_dict['Spread'])\n",
    "                        except ZeroDivisionError:\n",
    "                            stats_dict['IQR\\spread']='NA'\n",
    "                        try:\n",
    "                            stats_dict['IQR\\CI95']=stats_dict['IQR']/stats_dict['CI95']\n",
    "                        except ZeroDivisionError:\n",
    "                            stats_dict['IQR\\CI95']='NA'\n",
    "\n",
    "\n",
    "                    #Statistics the sensitivity based on endpoint_sum                     \n",
    "                    if (sensitivity_stats==1 and regular_stats==1):\n",
    "                        endpoint_sum=hdf5_file_MC_statistics['{}/endpoint_sum'.format(endpoint_group_path)]\n",
    "\n",
    "                        stats_dict['Spearmann rank correlation - coefficient']=stats.spearmanr(impact_method[1],endpoint_sum)[0]\n",
    "                        stats_dict['Spearmann rank correlation - pvalue']=stats.spearmanr(impact_method[1],endpoint_sum)[1]\n",
    "\n",
    "                        if np.isnan(stats_dict['Spearmann rank correlation - coefficient']):\n",
    "                            stats_dict['Spearmann rank correlation - coefficient']=0\n",
    "\n",
    "                        try:\n",
    "                            sum_spear_corr_endpoint[endpoint_name]=sum_spear_corr_endpoint[endpoint_name]+(stats_dict['Spearmann rank correlation - coefficient'])**2\n",
    "\n",
    "                        except:\n",
    "                            sum_spear_corr_endpoint[endpoint_name]=(stats_dict['Spearmann rank correlation - coefficient'])**2\n",
    "\n",
    "\n",
    "                        if 'LCI 0 LCIA 1' in uncertainty_level[0]:\n",
    "                            var_Y=hdf5_file_MC_statistics['{}/variance'.format(endpoint_group_path)][()]\n",
    "                            var_X=stats_dict['variance']\n",
    "                            stats_dict['Sensitivity index 1st order - midpoint to endpoint']=sensivity_index_1st_variance_ratio(var_Y,var_X)\n",
    "\n",
    "                        else:\n",
    "                            stats_dict['Sensitivity index 1st order - midpoint to endpoint']=sensivity_index_1st_estimate_smooth_curve(Y=endpoint_sum,X=impact_method[1],bin_size=bin_size)\n",
    "\n",
    "\n",
    "                        #print(str(stats_dict['Spearmann rank correlation - coefficient'])+' with sum '+str(sum_spear_corr_endpoint[endpoint_name]))\n",
    "\n",
    "\n",
    "                    #Store values\n",
    "                    impact_method_group_path='/{}/{}/{}'.format(uncertainty_level[0],act[0],impact_method[0])\n",
    "\n",
    "                    for indicator in stats_dict.keys():\n",
    "                        try:\n",
    "                            hdf5_file_MC_statistics.create_dataset('{}/{}'.format(impact_method_group_path,indicator),data=stats_dict[indicator])\n",
    "                        except:\n",
    "                            hdf5_file_MC_statistics['{}/{}'.format(impact_method_group_path,indicator)][...]=stats_dict[indicator]\n",
    "\n",
    "                \n",
    "                if (sensitivity_stats==1 and regular_stats==1):\n",
    "                    for impact_method in act[1].items():\n",
    "                        \n",
    "                        #If endpoint names are the second name in impact method tuples (...,...,...)\n",
    "                        #endpoint_name='{})'.format(impact_method[0].rsplit(',', 1)[0])\n",
    "                        endpoint_name='{},{})'.format(impact_method[0].split(',', 3)[0],impact_method[0].split(',', 3)[1])\n",
    "\n",
    "                        #If endpoint names are the first name in impact method tuples (...,...,...)\n",
    "                        #endpoint_name=impact_method[0].rsplit(',', 2)[0]+')'\n",
    "\n",
    "                        impact_method_group_path='/{}/{}/{}'.format(uncertainty_level[0],act[0],impact_method[0])\n",
    "\n",
    "                        #Calculating Contribution To Variance\n",
    "                        stats_dict={}\n",
    "                        stats_dict['Spearmann CTV midpoint to endpoint']=(hdf5_file_MC_statistics['{}/Spearmann rank correlation - coefficient'.format(impact_method_group_path)][()])**2/sum_spear_corr_endpoint[endpoint_name]\n",
    "\n",
    "\n",
    "                        #Store values\n",
    "                        for indicator in stats_dict.keys():\n",
    "                            try:\n",
    "                                hdf5_file_MC_statistics.create_dataset('{}/{}'.format(impact_method_group_path,indicator),data=stats_dict[indicator])\n",
    "                            except:\n",
    "                                hdf5_file_MC_statistics['{}/{}'.format(impact_method_group_path,indicator)][...]=stats_dict[indicator]\n",
    "\n",
    "                    \n",
    "                    \n",
    "    #Calculating Sensitivity index between uncertainty level for model_11 only \n",
    "    if (sensitivity_stats==1 and regular_stats==1):\n",
    "        for uncertainty_level in hdf5_file_MC_statistics.items():\n",
    "\n",
    "            if 'LCI 1 LCIA 1' in uncertainty_level[0]:\n",
    "\n",
    "                for act in uncertainty_level[1].items():                    \n",
    "\n",
    "                    for impact_method in act[1].items():\n",
    "\n",
    "                        impact_method_group_path='/{}/{}/{}'.format(uncertainty_level[0],act[0],impact_method[0])\n",
    "                        impact_method_group_path_LCI='/{}/{}/{}'.format('Uncertainty LCI 1 LCIA 0',act[0],impact_method[0])\n",
    "                        impact_method_group_path_LCIA='/{}/{}/{}'.format('Uncertainty LCI 0 LCIA 1',act[0],impact_method[0])\n",
    "\n",
    "                        var_Y=hdf5_file_MC_statistics['{}/variance'.format(impact_method_group_path)][()]\n",
    "                        var_X_LCI=hdf5_file_MC_statistics['{}/variance'.format(impact_method_group_path_LCI)][()]\n",
    "                        var_X_LCIA=hdf5_file_MC_statistics['{}/variance'.format(impact_method_group_path_LCIA)][()]\n",
    "\n",
    "                        stats_dict={}\n",
    "                        stats_dict['Sensitivity index 1st order - LCI parameters']=sensivity_index_1st_variance_ratio(var_Y,var_X_LCI)\n",
    "                        stats_dict['Sensitivity index 1st order - LCIA parameters']=sensivity_index_1st_variance_ratio(var_Y,var_X_LCIA)\n",
    "\n",
    "                        #Store values\n",
    "                        for indicator in stats_dict.keys():\n",
    "                            try:\n",
    "                                hdf5_file_MC_statistics.create_dataset('{}/{}'.format(impact_method_group_path,indicator),data=stats_dict[indicator])\n",
    "                            except:\n",
    "                                hdf5_file_MC_statistics['{}/{}'.format(impact_method_group_path,indicator)][...]=stats_dict[indicator]\n",
    "\n",
    "    return;\n",
    "\n",
    "\n",
    "def calculating_endpoint_stats_entire_database_aggregated_MC_results(hdf5_file_MC_LCA_results_path, dir_path_for_saving,bin_size):\n",
    "    \n",
    "    #Create and/or open the file for MC stats results\n",
    "    hdf5_file_MC_statistics=h5py.File(os.path.join(dir_path_for_saving,'MC_statistics_aggregated_results.hdf5'),'w-')\n",
    "    \n",
    "    #Open the MC LCA results file\n",
    "    hdf5_file_MC_LCA_results=h5py.File(hdf5_file_MC_LCA_results_path,'r')\n",
    "    \n",
    "    #Calculate stats --> only make sense if impact categories in hdf5_file_MC_LCA_results are endpoint per midpoint categories\n",
    "    calculating_endpoint_sum(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics)\n",
    "    calculating_endpoint_stats_indicators(hdf5_file_MC_LCA_results,hdf5_file_MC_statistics,bin_size)\n",
    "    \n",
    "    #Close hdf5 files\n",
    "    hdf5_file_MC_statistics.close()\n",
    "    hdf5_file_MC_LCA_results.close()\n",
    "    \n",
    "    return;    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdf5_file_MC_LCA_results_path=r\"D:\\Dropbox (MAGI)\\Dossiers professionnels\\Logiciels\\Brightway 2\\Monte Carlo results_with correlation\\Dependant LCA Monte Carlo - sector 4922\\LCA_Dependant_Monte_Carlo_aggregated_results_ALL.hdf5\"\n",
    "dir_path_for_saving=r\"D:\\Dropbox (MAGI)\\Dossiers professionnels\\Logiciels\\Brightway 2\\Monte Carlo results_with correlation\\Dependant LCA Monte Carlo - sector 4922\"\n",
    "\n",
    "bin_size=50\n",
    "calculating_endpoint_stats_entire_database_aggregated_MC_results(hdf5_file_MC_LCA_results_path, dir_path_for_saving,bin_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Second Step : comparing sensitivity indicators for each {sectors|impact method}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for the Page trend test\n",
    "Retrieved from https://github.com/jwcarr/PageTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PageTest v1.0.1\n",
    "# http://jwcarr.github.io/PageTest/\n",
    "#\n",
    "# Copyright (c) 2013-2015 Jon W. Carr\n",
    "# Licensed under the terms of the MIT License\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#   Run Page's test and return l, m, n, p, where l = Page's L statistic,\n",
    "#   m = number of replications, n = number of treatments, and p = p-value.\n",
    "\n",
    "def page_test(matrix, ascending=False, use_critical_values=False):\n",
    "  \"\"\"\n",
    "  Takes a matrix, with treatments along the columns and replications along the\n",
    "  rows, and returns Page's (1963) L statistic, along with its p-value.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  matrix : list\n",
    "      Data matrix (formated as a list of lists) with treatments along the\n",
    "      columns and replications along the rows.\n",
    "  ascending : bool, optional\n",
    "      Set to True if hypothesizing an ascending trend, False if hypothesizing\n",
    "      a descending trend (default: False).\n",
    "  use_critical_values : bool, optional\n",
    "      Set to True to use the critical values from Page (1963) rather than\n",
    "      compute an exact p-vaue (default: False).\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  L : float\n",
    "      Page's L statistic\n",
    "  m : int\n",
    "      Number of replications\n",
    "  n : int\n",
    "      Number of treatments\n",
    "  p : float or str\n",
    "      P-value\n",
    "  \"\"\"\n",
    "  validate_input(matrix, ascending, use_critical_values)\n",
    "  if ascending == True:\n",
    "    matrix = reverse_matrix(matrix)\n",
    "  m = len(matrix)\n",
    "  n = len(matrix[0])\n",
    "  l = page_l(matrix, m, n)\n",
    "  p = page_p(l, m, n, matrix, use_critical_values)\n",
    "  return l, m, n, p\n",
    "\n",
    "#   Calculate Page's L statistic.\n",
    "\n",
    "def page_l(matrix, m, n):\n",
    "  rank_matrix = []\n",
    "  for i in range(0, m):\n",
    "    rank = stats.rankdata(matrix[i])\n",
    "    rank_list = []\n",
    "    for j in range(0, n):\n",
    "      rank_list.append(rank[n-j-1])\n",
    "    rank_matrix.append(rank_list)\n",
    "  ranks = []\n",
    "  for i in range(0, n):\n",
    "    total = sum([row[i] for row in rank_matrix])\n",
    "    total *= i + 1\n",
    "    ranks.append(total)\n",
    "  return sum(ranks)\n",
    "\n",
    "#   Calculate a p-value for L using the appropriate method.\n",
    "\n",
    "def page_p(l, m, n, matrix, use_critical_values):\n",
    "  if use_critical_values == True:\n",
    "    try:\n",
    "      return page_critical_p(l, m, n)\n",
    "    except IndexError:\n",
    "      print('Large data matrix, so calculating exact p-value instead')\n",
    "  return page_exact_p(l, m, n, matrix)\n",
    "\n",
    "#   For small m and n, the exact p-value won't always agree with the critical\n",
    "#   values given in Page (1963, p. 220). If you prefer, you can use Page's critical\n",
    "#   values instead. This function looks up the critical values for m and n, and\n",
    "#   finds the significance level for L.\n",
    "\n",
    "def page_critical_p(l, m, n):\n",
    "  values = critical_values[n-3][m-2]\n",
    "  significance_levels = ['< 0.001', '< 0.01', '< 0.05']\n",
    "  for i in range(0, 3):\n",
    "    if l >= values[i] and values[i] != None:\n",
    "      return significance_levels[i]\n",
    "  return 'n.s.'\n",
    "\n",
    "#   Calculate the exact p-value using Eqation 4 in Page (1963)\n",
    "\n",
    "def page_exact_p(l, m, n, matrix):\n",
    "  # Calcualte L for the opposite trend\n",
    "  alt_l = page_l(reverse_matrix(matrix), m, n)\n",
    "  # If L for the opposite trend > L for the hypothesized trend, then the trend\n",
    "  # can't be significant ...\n",
    "  if alt_l > l:\n",
    "    # ... so return 'n.s.', otherwise the exact p-value could be misleading\n",
    "    # if the opposite trend happens to be significant\n",
    "    return 'n.s.'\n",
    "  chi_squared = ((12.0*l-3.0*m*n*(n+1.0)**2.0)**2.0)/(m*n**2.0*(n**2.0-1.0)*(n+1.0))\n",
    "  p_two_tailed = 1 - stats.chi2.cdf(chi_squared, 1)\n",
    "  # Return one-tailed p-value, since this is a one-tailed test\n",
    "  return p_two_tailed / 2.0\n",
    "\n",
    "#   Reverses the columns of a matrix\n",
    "\n",
    "def reverse_matrix(matrix):\n",
    "  return [[row[i] for i in reversed(range(len(matrix[0])))] for row in matrix]\n",
    "\n",
    "#   Validates the input arguments to catch common problems\n",
    "\n",
    "def validate_input(matrix, ascending, use_critical_values):\n",
    "  if type(matrix) != list:\n",
    "    raise TypeError('Matrix should be represented as Python lists')\n",
    "  for row_type in [type(row) for row in matrix]:\n",
    "    if row_type != list:\n",
    "      raise TypeError('Rows of the matrix should be represented as Python lists')\n",
    "  if len(set([len(row) for row in matrix])) != 1:\n",
    "    raise ValueError('Rows in matrix should have same length')\n",
    "  if len(matrix) < 2:\n",
    "    raise ValueError('Page\\'s test requires at least 2 replications')\n",
    "  if len(matrix[0]) < 3:\n",
    "    raise ValueError('Page\\'s test requires at least 3 treatments')\n",
    "  if type(ascending) != bool:\n",
    "    raise TypeError('The ascending argument should be set to True or False')\n",
    "  if type(use_critical_values) != bool:\n",
    "    raise TypeError('The use_critical_values argument should be set to True or False')\n",
    "\n",
    "critical_values = [[[None, None, 28], [None, 42, 41], [56, 55, 54], [70, 68, 66], [83, 81, 79], [96, 93, 91], [109, 106, 104], [121, 119, 116], [134, 131, 128], [147, 144, 141], [160, 156, 153], [172, 169, 165], [185, 181, 178], [197, 194, 190], [210, 206, 202], [223, 218, 215], [235, 231, 227], [248, 243, 239], [260, 256, 251]], [[None, 60, 58], [89, 87, 84], [117, 114, 111], [145, 141, 137], [172, 167, 163], [198, 193, 189], [225, 220, 214], [252, 246, 240], [278, 272, 266], [305, 298, 292], [331, 324, 317]], [[109, 106, 103], [160, 155, 150], [210, 204, 197], [259, 251, 244], [307, 299, 291], [355, 346, 338], [403, 393, 384], [451, 441, 431], [499, 487, 477], [546, 534, 523], [593, 581, 570]], [[178, 173, 166], [260, 252, 244], [341, 331, 321], [420, 409, 397], [499, 486, 474], [577, 563, 550], [655, 640, 625], [733, 717, 701], [811, 793, 777], [888, 869, 852], [965, 946, 928]], [[269, 261, 252], [394, 382, 370], [516, 501, 487], [637, 620, 603], [757, 737, 719], [876, 855, 835], [994, 972, 950], [1113, 1088, 1065], [1230, 1205, 1180], [1348, 1321, 1295], [1465, 1437, 1410]], [[388, 376, 362], [567, 549, 532], [743, 722, 701], [917, 893, 869], [1090, 1063, 1037], [1262, 1232, 1204], [1433, 1401, 1371], [1603, 1569, 1537], [1773, 1736, 1703], [1943, 1905, 1868], [2112, 2072, 2035]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Wilcoxon signed-rank test one tailed\n",
    "\n",
    "Original code from scipy: https://github.com/scipy/scipy/blob/v0.19.1/scipy/stats/morestats.py#L2328-L2425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import math\n",
    "import warnings #\n",
    "#from collections import namedtuple\n",
    "\n",
    "import numpy as np #\n",
    "from numpy import (isscalar, r_, log, around, unique, asarray,\n",
    "                   zeros, arange, sort, amin, amax, any, atleast_1d,\n",
    "                   sqrt, ceil, floor, array, poly1d, compress,\n",
    "                   pi, exp, ravel, count_nonzero, sin, cos, arctan2, hypot) #\n",
    "#from numpy.testing.decorators import setastest\n",
    "\n",
    "#from scipy._lib.six import string_types\n",
    "#from scipy import optimize\n",
    "#from scipy import special\n",
    "#from . import statlib\n",
    "from scipy import stats #from . import stats #\n",
    "from scipy.stats import find_repeats #from .stats import find_repeats, _contains_nan #\n",
    "#from .contingency import chi2_contingency\n",
    "from scipy.stats import distributions #from . import distributions #\n",
    "#from ._distn_infrastructure import rv_generic\n",
    "\n",
    "\n",
    "__all__ = ['mvsdist',\n",
    "           'bayes_mvs', 'kstat', 'kstatvar', 'probplot', 'ppcc_max', 'ppcc_plot',\n",
    "           'boxcox_llf', 'boxcox', 'boxcox_normmax', 'boxcox_normplot',\n",
    "           'shapiro', 'anderson', 'ansari', 'bartlett', 'levene', 'binom_test',\n",
    "           'fligner', 'mood', 'wilcoxon', 'median_test',\n",
    "           'pdf_fromgamma', 'circmean', 'circvar', 'circstd', 'anderson_ksamp'\n",
    "           ]\n",
    "\n",
    "\n",
    "#Mean = namedtuple('Mean', ('statistic', 'minmax'))\n",
    "#Variance = namedtuple('Variance', ('statistic', 'minmax'))\n",
    "#Std_dev = namedtuple('Std_dev', ('statistic', 'minmax'))\n",
    "\n",
    "\n",
    "def wilcoxon_one_tailed(x, y=None, zero_method=\"pratt\", correction=False):\n",
    "    \"\"\"\n",
    "    Calculate the Wilcoxon signed-rank test.\n",
    "    The Wilcoxon signed-rank test tests the null hypothesis that two\n",
    "    related paired samples come from the same distribution. In particular,\n",
    "    it tests whether the distribution of the differences x - y is symmetric\n",
    "    about zero. It is a non-parametric version of the paired T-test.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        The first set of measurements.\n",
    "    y : array_like, optional\n",
    "        The second set of measurements.  If `y` is not given, then the `x`\n",
    "        array is considered to be the differences between the two sets of\n",
    "        measurements.\n",
    "    zero_method : string, {\"pratt\", \"wilcox\", \"zsplit\"}, optional\n",
    "        \"pratt\":\n",
    "            Pratt treatment: includes zero-differences in the ranking process\n",
    "            (more conservative)\n",
    "        \"wilcox\":\n",
    "            Wilcox treatment: discards all zero-differences\n",
    "        \"zsplit\":\n",
    "            Zero rank split: just like Pratt, but spliting the zero rank\n",
    "            between positive and negative ones\n",
    "    correction : bool, optional\n",
    "        If True, apply continuity correction by adjusting the Wilcoxon rank\n",
    "        statistic by 0.5 towards the mean value when computing the\n",
    "        z-statistic.  Default is False.\n",
    "    Returns\n",
    "    -------\n",
    "    statistic : float\n",
    "        The sum of the ranks of the differences above or below zero, whichever\n",
    "        is smaller.\n",
    "    pvalue : float\n",
    "        The two-sided p-value for the test.\n",
    "    Notes\n",
    "    -----\n",
    "    Because the normal approximation is used for the calculations, the\n",
    "    samples used should be large.  A typical rule is to require that\n",
    "    n > 20.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n",
    "    \"\"\"\n",
    "\n",
    "    if zero_method not in [\"wilcox\", \"pratt\", \"zsplit\"]:\n",
    "        raise ValueError(\"Zero method should be either 'wilcox' \"\n",
    "                         \"or 'pratt' or 'zsplit'\")\n",
    "\n",
    "    if y is None:\n",
    "        d = asarray(x)\n",
    "    else:\n",
    "        x, y = map(asarray, (x, y))\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError('Unequal N in wilcoxon.  Aborting.')\n",
    "        d = x - y\n",
    "\n",
    "    if zero_method == \"wilcox\":\n",
    "        # Keep all non-zero differences\n",
    "        d = compress(np.not_equal(d, 0), d, axis=-1)\n",
    "\n",
    "    count = len(d)\n",
    "    if count < 10:\n",
    "        warnings.warn(\"Warning: sample size too small for normal approximation.\")\n",
    "\n",
    "    r = stats.rankdata(abs(d))\n",
    "    r_plus = np.sum((d > 0) * r, axis=0)\n",
    "    r_minus = np.sum((d < 0) * r, axis=0)\n",
    "\n",
    "    if zero_method == \"zsplit\":\n",
    "        r_zero = np.sum((d == 0) * r, axis=0)\n",
    "        r_plus += r_zero / 2.\n",
    "        r_minus += r_zero / 2.\n",
    "\n",
    "    T = min(r_plus, r_minus)\n",
    "    \n",
    "    #to determine the direction of the test\n",
    "    if r_plus>r_minus:\n",
    "        test_sign='>0'\n",
    "        \n",
    "    if r_plus<r_minus:\n",
    "        test_sign='<0'\n",
    "    \n",
    "    mn = count * (count + 1.) * 0.25\n",
    "    se = count * (count + 1.) * (2. * count + 1.)\n",
    "\n",
    "    if zero_method == \"pratt\":\n",
    "        r = r[d != 0]\n",
    "\n",
    "    replist, repnum = find_repeats(r)\n",
    "    if repnum.size != 0:\n",
    "        # Correction for repeated elements.\n",
    "        se -= 0.5 * (repnum * (repnum * repnum - 1)).sum()\n",
    "\n",
    "    se = sqrt(se / 24)\n",
    "    correction = 0.5 * int(bool(correction)) * np.sign(T - mn)\n",
    "    z = (T - mn - correction) / se\n",
    "    prob = 2. * distributions.norm.sf(abs(z))\n",
    "    prob=prob/2 #for a one tailed test\n",
    "\n",
    "    return (T,test_sign, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.random.normal(1,1.06,100)\n",
    "Y=np.random.lognormal(2,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549.0, '<0', 5.4484088077383994e-12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon_one_tailed(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for comparing sensivity index by sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "import collections\n",
    "#import statistics\n",
    "\n",
    "\n",
    "#for a specific impact category with stats indicator for all activities of a sector\n",
    "def test_LCI_LCIA_ranking(stats_LCI,stats_LCIA):\n",
    "    \n",
    "    T,test_sign,prob=wilcoxon_one_tailed(stats_LCI,stats_LCIA)\n",
    "    \n",
    "    if prob<0.001:\n",
    "        test_significance='<0.001'\n",
    "        \n",
    "    elif prob<0.01:\n",
    "        test_significance='<0.01'\n",
    "        \n",
    "    elif prob<0.05:\n",
    "        test_significance='<0.05'\n",
    "        \n",
    "    else:\n",
    "        test_significance='Not significant'\n",
    "        \n",
    "    if test_sign=='<0':\n",
    "        test_sign_trad='LCI < LCIA'\n",
    "        \n",
    "    elif test_sign=='>0':\n",
    "        test_sign_trad='LCI > LCIA'\n",
    "        \n",
    "    return (test_sign_trad,test_significance);\n",
    "\n",
    "#for a specific endpoint impact category with stats indicator for all activities of a sector\n",
    "def test_IC_ranking(stats_ICs):\n",
    "    \n",
    "    #here we suppose that stats_IC are {impact_method:value of stats_IC for activities}\n",
    "    stats_ICs_mean_dict={np.nanmean(stats_IC):key for key,stats_IC in stats_ICs.items()}\n",
    "    \n",
    "    #The order prediction is based on the mean value\n",
    "    sorted_stats_ICs_mean_dict=collections.OrderedDict(sorted(stats_ICs_mean_dict.items(),reverse=True))\n",
    "    \n",
    "    #Gathering data in the good format\n",
    "    data=[]\n",
    "    descending_order=[]\n",
    "    \n",
    "    for order,key in sorted_stats_ICs_mean_dict.items():\n",
    "        data.append(stats_ICs[key])\n",
    "        descending_order.append(key)\n",
    "        \n",
    "    data=np.transpose(data).tolist()\n",
    "    \n",
    "    #Perform test and get the test significance\n",
    "    l,m,n,prob=page_test(data)\n",
    "    \n",
    "    if prob=='n.s.':\n",
    "        test_significance='Not significant'\n",
    "        \n",
    "    else:\n",
    "        if prob<0.001:\n",
    "            test_significance='<0.001'\n",
    "\n",
    "        elif prob<0.01:\n",
    "            test_significance='<0.01'\n",
    "\n",
    "        elif prob<0.05:\n",
    "            test_significance='<0.05'\n",
    "\n",
    "        else:\n",
    "            test_significance='Not significant'\n",
    "        \n",
    "    return (descending_order,test_significance);\n",
    "\n",
    "\n",
    "\n",
    "def gathering_stats_from_sector_activities(activity_code_list,hdf5_file_MC_statistics,stats_name):\n",
    "    \n",
    "    for_LC_ranking={}\n",
    "    for_IC_ranking={}\n",
    "    \n",
    "    stats_MP_to_EP_name='{}{}'.format(stats_name,' - midpoint to endpoint')\n",
    "    stats_LCI_name='{}{}'.format(stats_name,' - LCI parameters')\n",
    "    stats_LCIA_name='{}{}'.format(stats_name,' - LCIA parameters')\n",
    "    \n",
    "    for uncertainty_level in hdf5_file_MC_statistics.items():\n",
    "        \n",
    "        for_IC_ranking[uncertainty_level[0]]={}\n",
    "                \n",
    "        for act in activity_code_list:\n",
    "\n",
    "            act_group=hdf5_file_MC_statistics['{}/{}'.format(uncertainty_level[0],act)]\n",
    "\n",
    "            for impact_method in act_group.items():\n",
    "\n",
    "                endpoint_name='{},{}'.format(impact_method[0].split(',', 3)[0],impact_method[0].split(',', 3)[1])\n",
    "                impact_method_group_path='/{}/{}/{}'.format(uncertainty_level[0],act,impact_method[0])\n",
    "\n",
    "                if impact_method[0]!=endpoint_name:\n",
    "                    \n",
    "                    try:\n",
    "                        act_stats_endpoint=for_IC_ranking[uncertainty_level[0]][endpoint_name]\n",
    "                    except KeyError:\n",
    "                        for_IC_ranking[uncertainty_level[0]][endpoint_name]={}\n",
    "                        act_stats_endpoint=for_IC_ranking[uncertainty_level[0]][endpoint_name]\n",
    "                        \n",
    "                    try:\n",
    "                        act_stats=act_stats_endpoint[impact_method[0]]\n",
    "                    except KeyError:\n",
    "                        act_stats_endpoint[impact_method[0]]=[]\n",
    "                        act_stats=act_stats_endpoint[impact_method[0]]\n",
    "                    \n",
    "                    act_stat=hdf5_file_MC_statistics['{}/{}'.format(impact_method_group_path,stats_MP_to_EP_name)][()]\n",
    "                    act_stats.append(act_stat)\n",
    "                    \n",
    "                if 'LCI 1 LCIA 1' in uncertainty_level[0]:\n",
    "                                       \n",
    "                    try:\n",
    "                        act_stats_LCI=for_LC_ranking[impact_method[0]]['LCI']\n",
    "                        act_stats_LCIA=for_LC_ranking[impact_method[0]]['LCIA']\n",
    "                    except KeyError:\n",
    "                        for_LC_ranking[impact_method[0]]={}\n",
    "                        for_LC_ranking[impact_method[0]]['LCI']=[]\n",
    "                        for_LC_ranking[impact_method[0]]['LCIA']=[]\n",
    "                        act_stats_LCI=for_LC_ranking[impact_method[0]]['LCI']\n",
    "                        act_stats_LCIA=for_LC_ranking[impact_method[0]]['LCIA']\n",
    "                    \n",
    "                    act_stat_LCI=hdf5_file_MC_statistics['{}/{}'.format(impact_method_group_path,stats_LCI_name)][()]\n",
    "                    act_stat_LCIA=hdf5_file_MC_statistics['{}/{}'.format(impact_method_group_path,stats_LCIA_name)][()]\n",
    "                    act_stats_LCI.append(act_stat_LCI)\n",
    "                    act_stats_LCIA.append(act_stat_LCIA)\n",
    "                    \n",
    "    return {'For LC ranking':for_LC_ranking,'For IC ranking':for_IC_ranking};\n",
    "\n",
    "\n",
    "\n",
    "def test_results_from_sector_activities(activity_code_list,hdf5_file_MC_statistics,stats_name):\n",
    "    \n",
    "    data_dict=gathering_stats_from_sector_activities(activity_code_list,hdf5_file_MC_statistics,stats_name)\n",
    "    \n",
    "    data_LC_ranking=data_dict['For LC ranking']\n",
    "    data_IC_ranking=data_dict['For IC ranking']\n",
    "    \n",
    "    results_LC_ranking={}\n",
    "    results_IC_ranking={}\n",
    "    \n",
    "    for impact_method in data_LC_ranking.keys():\n",
    "        \n",
    "        stats_LCI=data_LC_ranking[impact_method]['LCI']\n",
    "        stats_LCIA=data_LC_ranking[impact_method]['LCIA']\n",
    "        \n",
    "        results_LC_ranking[impact_method]=test_LCI_LCIA_ranking(stats_LCI,stats_LCIA)\n",
    "        \n",
    "    for uncertainty_level in data_IC_ranking.keys():\n",
    "        \n",
    "        results_IC_ranking[uncertainty_level]={}\n",
    "        \n",
    "        for endpoint_impact_method in data_IC_ranking[uncertainty_level].keys():\n",
    "                        \n",
    "            stats_ICs=data_IC_ranking[uncertainty_level][endpoint_impact_method]\n",
    "            results_IC_ranking[uncertainty_level][endpoint_impact_method]=test_IC_ranking(stats_ICs)\n",
    "            \n",
    "    return [results_LC_ranking,results_IC_ranking];\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import brightway2 as bw\n",
    "bw.projects.set_current('iw_integration')\n",
    "DB_eiv33=bw.Database('ecoinvent 3.3 cutoff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_19a=[act for act in DB_eiv33 if '4922' in str(act['classifications'])]\n",
    "code_act_19a=[act['code'] for act in act_19a]\n",
    "name_act_19a=[act['name'] for act in act_19a]\n",
    "info_act_19a={act['code']:{'name':act['name'],'location':act['location']} for act in act_19a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_name='Sensitivity index 1st order'\n",
    "activity_code_list=code_act_19a\n",
    "\n",
    "filepath=r\"D:\\Dropbox (MAGI)\\Dossiers professionnels\\Logiciels\\Brightway 2\\Monte Carlo results_with correlation\\Dependant LCA Monte Carlo - sector 4922\"\n",
    "hdf5_file_MC_statistics=h5py.File(os.path.join(filepath,'MC_statistics_aggregated_results.hdf5'),'r')\n",
    "\n",
    "[results_LC_ranking,results_IC_ranking]=test_results_from_sector_activities(activity_code_list,hdf5_file_MC_statistics,stats_name)\n",
    "\n",
    "hdf5_file_MC_statistics.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Format data for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_file_MC_statistics=h5py.File(os.path.join(filepath,'MC_statistics_aggregated_results.hdf5'),'r')\n",
    "gather_dict=gathering_stats_from_sector_activities(activity_code_list,hdf5_file_MC_statistics,stats_name)\n",
    "hdf5_file_MC_statistics.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LC_ranking_data_dict=gather_dict['For LC ranking']\n",
    "IC_ranking_data_dict=gather_dict['For IC ranking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_dict_LC_ranking_data(data_dict,code_act):\n",
    "\n",
    "    data_dict_simple={}\n",
    "\n",
    "    for ic in data_dict.keys():\n",
    "        for lc_level in data_dict[ic].keys():\n",
    "            i=0\n",
    "            for si in data_dict[ic][lc_level]:\n",
    "                key=lc_level+' - '+code_act[i]+' - '+ic\n",
    "                data_dict_simple[key]={'impact category':ic,\n",
    "                                       'Activity':code_act[i],\n",
    "                                               'LC level':lc_level,\n",
    "                                               'Sensitivity index value':si}\n",
    "                i=i+1\n",
    "\n",
    "    return data_dict_simple;\n",
    "\n",
    "def simple_dict_IC_ranking_data(data_dict,code_act):\n",
    "\n",
    "    data_dict_simple={}\n",
    "\n",
    "    for lc_level in data_dict.keys():\n",
    "        for ic_endpoint in data_dict[lc_level].keys():\n",
    "            for ic_midpoint in data_dict[lc_level][ic_endpoint].keys():\n",
    "                i=0\n",
    "                for si in data_dict[lc_level][ic_endpoint][ic_midpoint]:\n",
    "                    key=lc_level+' - '+code_act[i]+' - '+ic_endpoint+' - '+ic_midpoint\n",
    "                    data_dict_simple[key]={'LC level':lc_level,\n",
    "                                                   'Activity':code_act[i],\n",
    "                                                   'Endpoint IC':ic_endpoint,\n",
    "                                                   'Midpoint IC':ic_midpoint,\n",
    "                                                   'Sensitivity index value':si}\n",
    "                    i=i+1\n",
    "\n",
    "    return data_dict_simple;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LC_ranking_data_dict_simple=simple_dict_LC_ranking_data(LC_ranking_data_dict,code_act_19a)\n",
    "IC_ranking_data_dict_simple=simple_dict_IC_ranking_data(IC_ranking_data_dict,code_act_19a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format results for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_dict_LC_ranking_results(results_LC_ranking):\n",
    "    results_LC_ranking_dict_simple={}\n",
    "\n",
    "    for ic in results_LC_ranking.keys():\n",
    "        results_LC_ranking_dict_simple[ic]={'LC ranking':results_LC_ranking[ic][0],\n",
    "                                          'Level of significance':results_LC_ranking[ic][1]}\n",
    "        \n",
    "    return results_LC_ranking_dict_simple;\n",
    "\n",
    "\n",
    "def simple_dict_IC_ranking_results(results_IC_ranking):\n",
    "    results_IC_ranking_dict_simple={}\n",
    "\n",
    "    for lc_level in results_IC_ranking.keys():\n",
    "        for ic_endpoint in results_IC_ranking[lc_level].keys():\n",
    "            list_ic=results_IC_ranking[lc_level][ic_endpoint][0]\n",
    "            test_significance=results_IC_ranking[lc_level][ic_endpoint][1]\n",
    "            j=1\n",
    "\n",
    "            for ic_midpoint in list_ic:\n",
    "                key=lc_level+' - '+str(j)+' - '+ic_endpoint\n",
    "                results_IC_ranking_dict_simple[key]={'LC level':lc_level,\n",
    "                                                   'Endpoint IC':ic_endpoint,\n",
    "                                                   'Midpoint IC':ic_midpoint,\n",
    "                                                   'Level of significance':test_significance,\n",
    "                                                     'order':j}\n",
    "\n",
    "                j=j+1\n",
    "                \n",
    "    return results_IC_ranking_dict_simple;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_IC_ranking_dict_simple=simple_dict_IC_ranking_results(results_IC_ranking)\n",
    "results_LC_ranking_dict_simple=simple_dict_LC_ranking_results(results_LC_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data and results with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_and_data_dict={'SI1_sector19a_data':gather_dict,'SI1_sector19a_test_results':{'results_LC_ranking':results_LC_ranking,'results_IC_ranking':results_IC_ranking}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle_save(file_path_root,object_name_to_save,file_name):\n",
    "    \n",
    "    complete_file_path=file_path_root+'\\\\'+file_name+'.p'\n",
    "    \n",
    "    pickle.dump( object_name_to_save, open( complete_file_path, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path_root=r'D:\\Dropbox (MAGI)\\Dossiers professionnels\\Logiciels\\Brightway 2\\Monte Carlo results_with correlation\\Dependant LCA Monte Carlo - sector 4922\\SI1 test results'\n",
    "object_name_to_save=results_and_data_dict\n",
    "file_name='results_and_data_dict_sector19a_withoutLandTransfo'\n",
    "\n",
    "pickle_save(file_path_root,object_name_to_save,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle_load(file_path_root,file_name):\n",
    "    \n",
    "    complete_file_path=file_path_root+'\\\\'+file_name+'.p'\n",
    "    \n",
    "    return pickle.load( open( complete_file_path, \"rb\" ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path_root=r'D:\\Dropbox (MAGI)\\Dossiers professionnels\\Logiciels\\Brightway 2\\Monte Carlo results_with correlation\\Dependant LCA Monte Carlo - sector 4922\\SI1 test results'\n",
    "file_name='results_and_data_dict_sector4922'\n",
    "\n",
    "results_and_data_dict=pickle_load(file_path_root,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LC_ranking_data_dict=results_and_data_dict['SI1_sector19a_data']['For LC ranking']\n",
    "IC_ranking_data_dict=results_and_data_dict['SI1_sector19a_data']['For IC ranking']\n",
    "\n",
    "results_LC_ranking=results_and_data_dict['SI1_sector19a_test_results']['results_LC_ranking']\n",
    "results_IC_ranking=results_and_data_dict['SI1_sector19a_test_results']['results_IC_ranking']\n",
    "\n",
    "LC_ranking_data_dict_simple=simple_dict_LC_ranking_data(LC_ranking_data_dict,code_act_19a)\n",
    "IC_ranking_data_dict_simple=simple_dict_IC_ranking_data(IC_ranking_data_dict,code_act_19a)\n",
    "\n",
    "results_IC_ranking_dict_simple=simple_dict_IC_ranking_results(results_IC_ranking)\n",
    "results_LC_ranking_dict_simple=simple_dict_LC_ranking_results(results_LC_ranking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data in csv with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath_root=r'D:\\Dropbox (MAGI)\\Dossiers professionnels\\Logiciels\\Brightway 2\\Monte Carlo results_with correlation\\Dependant LCA Monte Carlo - sector 4922\\SI1 test results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_name_sector=pd.DataFrame.from_dict(info_act_19a,orient='index')\n",
    "df_name_sector.to_csv(sep=';',path_or_buf=filepath_root+'\\Activites_sector_c19a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data_LC_ranking=pd.DataFrame.from_dict(LC_ranking_data_dict_simple,orient='index')\n",
    "df_data_LC_ranking.to_csv(sep=';',path_or_buf=filepath_root+'\\Data_LC_ranking_sector_c19a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_IC_ranking=pd.DataFrame.from_dict(IC_ranking_data_dict_simple,orient='index')\n",
    "df_data_IC_ranking.to_csv(sep=';',path_or_buf=filepath_root+'\\Data_IC_ranking_sector_c19a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results_IC_ranking=pd.DataFrame.from_dict(results_IC_ranking_dict_simple,orient='index')\n",
    "df_results_IC_ranking.to_csv(sep=';',path_or_buf=filepath_root+'\\Results_IC_ranking_sector_c19a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results_LC_ranking=pd.DataFrame.from_dict(results_LC_ranking_dict_simple,orient='index')\n",
    "df_results_LC_ranking.to_csv(sep=';',path_or_buf=filepath_root+'\\Results_LC_ranking_sector_c19a.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
