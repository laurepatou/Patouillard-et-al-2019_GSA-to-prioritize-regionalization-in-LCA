{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to perform a database wide Monte Carlo (LCI and LCA) per sector in ecoinvent v3 database and IMPACT world+ method\n",
    "\n",
    "Work with Python 3.6, Brightway 2 and HDF5 storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCI Monte Carlo\n",
    "\n",
    "To be run in console as Database_wide_Monte_Carlo_LCI_calculation_with_correlation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from brightway2 import *\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from scipy.sparse.linalg import factorized, spsolve\n",
    "from scipy import sparse\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import click\n",
    "import scipy as sp\n",
    "import h5py\n",
    "import stats_arrays\n",
    "\n",
    "\"\"\"\n",
    "Used to generate uncertainty information at the database level.\n",
    "For each iteration:\n",
    "- New values for uncertain parameters of the technosphere (A) and biosphere (B) matrices are generated\n",
    "- Cradle-to-gate LCI results are calculated for all potential output of the LCI database\n",
    "\n",
    "The following is stored in a specified directory: \n",
    "- All values of the A and B matrices\n",
    "- For each functional unit: \n",
    "    - the supply array (aka the scaling vector)\n",
    "    - the life cycle inventory\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "##################\n",
    "# HDF5 functions #\n",
    "##################\n",
    "\n",
    "# All those functions work based on LCA objects from Brightway\n",
    "\n",
    "# function create a group containing all the information of a csr matrix scipy  \n",
    "\n",
    "def csr_matrix_to_hdf5(csr,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    csr_size=csr.nnz\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('data',data=csr.data,compression=\"gzip\",dtype=np.float32)\n",
    "    group.create_dataset('indptr',data=csr.indptr,compression=\"gzip\")\n",
    "    group.create_dataset('indices',data=csr.indices,compression=\"gzip\")\n",
    "    group.create_dataset('shape',data=csr.shape)\n",
    "\n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _dict: biosphere_dict, activity_dict, product_dict\n",
    "def LCA_dict_to_hdf5(LCA_dict,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    ###### WARNING : Modify the builder because _dict items are like \n",
    "    #####('ecoinvent 3.3 cutoff', 'c533b046462b6c56a5636ca177347c48'): 35\n",
    "    #### Use .decode('UTF-8') to convert keys_1_list items for bytes to str\n",
    "    \n",
    "    \n",
    "    keys_0=np.string_([key[0] for key in LCA_dict.keys()][0])\n",
    "    keys_1_list=np.string_([key[1] for key in LCA_dict.keys()])\n",
    "    items_list=[item for item in LCA_dict.values()]\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('keys_1',data=keys_1_list,compression=\"gzip\")\n",
    "    group.create_dataset('values',data=items_list,compression=\"gzip\")\n",
    "    group.create_dataset('keys_0',data=keys_0)\n",
    "\n",
    "    return;\n",
    "\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _***_dict: _biosphere_dict, _activity_dict, _product_dict\n",
    "def _LCA_dict_to_hdf5(LCA_dict,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)  \n",
    "    \n",
    "    keys=[key for key in LCA_dict.keys()]\n",
    "    values=[item for item in LCA_dict.values()]\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('keys',data=keys,compression=\"gzip\")\n",
    "    group.create_dataset('values',data=values,compression=\"gzip\")\n",
    "\n",
    "    return;\n",
    "\n",
    "\n",
    "\n",
    "#Function to write csr matrix, _dict from LCA objects and any numpy.ndarray\n",
    "\n",
    "def write_LCA_obj_to_HDF5_file(LCA_obj,hdf5_file,group_path):\n",
    "    \n",
    "    dict_names_to_check=['biosphere_dict', 'activity_dict', 'product_dict']\n",
    "    _dict_names_to_check=['_biosphere_dict', '_activity_dict', '_product_dict']\n",
    "    \n",
    "    #If object = A or B matrix\n",
    "    if type(LCA_obj)==sp.sparse.csr.csr_matrix:\n",
    "        #csr_matrix_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "        #### Direct copy of the function because the call to the function does not work... --> Works now!\n",
    "        # Retrieve or create groups and subgroups\n",
    "        group=hdf5_file.require_group(group_path)\n",
    "\n",
    "        # Create datasets containing values of csr matrix\n",
    "        group.create_dataset('data',data=LCA_obj.data,compression=\"gzip\",dtype=np.float32)\n",
    "        group.create_dataset('indptr',data=LCA_obj.indptr,compression=\"gzip\")\n",
    "        group.create_dataset('indices',data=LCA_obj.indices,compression=\"gzip\")\n",
    "        group.create_dataset('shape',data=LCA_obj.shape)\n",
    "\n",
    "        ######\n",
    "        \n",
    "        \n",
    "    #If object = _***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in _dict_names_to_check:\n",
    "        _LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)    \n",
    "        \n",
    "    \n",
    "    #If object = ***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in dict_names_to_check:\n",
    "        LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #store as float32 if type is float64 to save space\n",
    "        if LCA_obj.dtype == np.dtype('float64'):\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\",dtype=np.float32)\n",
    "            \n",
    "        else:\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\")\n",
    "            \n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "def h5py_dataset_iterator(g, prefix=''):\n",
    "    for key in g.keys():\n",
    "        item = g[key]\n",
    "        path = '{}/{}'.format(prefix, key)\n",
    "        if isinstance(item, h5py.Dataset): # test for dataset\n",
    "            yield (path, item)\n",
    "        elif isinstance(item, h5py.Group): # test for group (go down)\n",
    "            yield from h5py_dataset_iterator(item, path)\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Dependant LCI Monte Carlo functions #\n",
    "#######################################\n",
    "\n",
    "\n",
    "#Change for DirectSolvingMonteCarloLCA(MonteCarloLCA, DirectSolvingMixin)?\n",
    "class DirectSolvingMonteCarloLCA(MonteCarloLCA, DirectSolvingMixin):\n",
    "    pass\n",
    "\n",
    "#Clean the HDF5 file with LCI MC results for not complete iterations (i.e. not all activities calculated for the last iteration)\n",
    "def clean_hdf5_file_MC_results(hdf5_file_MC_results,worker_id):\n",
    "    \n",
    "    complete_iterations=int(hdf5_file_MC_results.attrs['Number of complete iterations'])\n",
    "    \n",
    "    #Clean techno and bio matrix for not complete iterations\n",
    "    techno_group=hdf5_file_MC_results['/technosphere_matrix']\n",
    "    bio_group=hdf5_file_MC_results['/biosphere_matrix']\n",
    "    techno_iterations=len(techno_group)\n",
    "    bio_iterations=len(bio_group)\n",
    "\n",
    "    print('--Number of iterations for A and B retrieved for worker {}'.format(worker_id))\n",
    "\n",
    "    if techno_iterations!=complete_iterations:\n",
    "        iteration_name_to_delete=techno_iterations-1\n",
    "        del techno_group[str(iteration_name_to_delete)]\n",
    "        #print('--Incomplete iterations for A removed for worker {}'.format(worker_id))\n",
    "\n",
    "    if bio_iterations!=complete_iterations:\n",
    "        iteration_name_to_delete=bio_iterations-1\n",
    "        del bio_group[str(iteration_name_to_delete)]\n",
    "        #print('--Incomplete iterations for B removed for worker {}'.format(worker_id))\n",
    "\n",
    "    #Clean supply arrays for not complete iterations\n",
    "    supply_array_group=hdf5_file_MC_results['/supply_array']\n",
    "\n",
    "    print('--Number of iterations for supply_array retrieved for worker {}'.format(worker_id))\n",
    "\n",
    "    for act in supply_array_group:\n",
    "        supply_act_iterations=len(supply_array_group[act])\n",
    "        if supply_act_iterations!=complete_iterations:\n",
    "            iteration_name_to_delete=supply_act_iterations-1\n",
    "            del supply_array_group[act][str(iteration_name_to_delete)]\n",
    "            #print('--Incomplete iterations for supply_array removed for worker {} for activity {}'.format(worker_id,act))\n",
    "            \n",
    "    return;\n",
    "\n",
    "#Create a file that gather all MC results and Useful info in one file\n",
    "def gathering_MC_results_in_one_hdf5_file(path_for_saving):\n",
    "    \n",
    "    \n",
    "    #Create the gathering file\n",
    "    hdf5_file_all_MC_results_path=os.path.join(path_for_saving,'LCI_Dependant_Monte_Carlo_results_ALL.hdf5')\n",
    "    hdf5_file_all_MC_results=h5py.File(hdf5_file_all_MC_results_path,'w-')\n",
    "    \n",
    "    #Retrieve child file paths\n",
    "    child_hdf5_file_paths = [os.path.join(path_for_saving,fn) for fn in next(os.walk(path_for_saving))[2] if '.hdf5' in fn]\n",
    "    child_MC_results_paths=[path for path in child_hdf5_file_paths if 'LCI_Dependant_Monte_Carlo_results_worker' in path]\n",
    "    child_DB_info_paths=[path for path in child_hdf5_file_paths if 'Useful_info_per_DB' in path]\n",
    "    \n",
    "    #Gathering MC results\n",
    "    complete_iterations=0\n",
    "    \n",
    "    for child_file_path in child_MC_results_paths:\n",
    "        \n",
    "        worker_id=child_file_path.rsplit('LCI_Dependant_Monte_Carlo_results_worker', 1)[1]\n",
    "        \n",
    "        start_1 = time.time()\n",
    "        \n",
    "        #Clean incomplete iterations before gathering \n",
    "        child_hdf5_file=h5py.File(child_file_path,'r+')\n",
    "        clean_hdf5_file_MC_results(child_hdf5_file,worker_id)\n",
    "        child_hdf5_file.close()\n",
    "        \n",
    "        end_1 = time.time()\n",
    "        print(\"Cleaning in {} secondes for entire DB for worker {}\".format(end_1 - start_1,worker_id)) \n",
    "\n",
    "        start_2 = time.time()\n",
    "        \n",
    "        child_hdf5_file=h5py.File(child_file_path,'r')\n",
    "    \n",
    "        for (child_dataset_path, dset) in h5py_dataset_iterator(child_hdf5_file):\n",
    "            \n",
    "            #For supply array\n",
    "            if 'supply_array' in child_dataset_path:\n",
    "            \n",
    "                #Create similar path for the master file\n",
    "                child_iteration_name=child_dataset_path.rsplit('/', 1)[1]\n",
    "                master_iteration_name=str(int(child_iteration_name)+complete_iterations)\n",
    "                master_dataset_path='{}/{}'.format(child_dataset_path.rsplit('/', 1)[0],master_iteration_name)\n",
    "\n",
    "                #Link child data to master file\n",
    "                hdf5_file_all_MC_results[master_dataset_path] = h5py.ExternalLink(child_file_path, child_dataset_path)\n",
    "             \n",
    "            #For A and B matrix\n",
    "            else:\n",
    "                #Create similar path for the master file\n",
    "                child_iteration_name=child_dataset_path.rsplit('/', 2)[1]\n",
    "                master_iteration_name=str(int(child_iteration_name)+complete_iterations)\n",
    "                master_dataset_path='{}/{}/{}'.format(child_dataset_path.rsplit('/', 2)[0],master_iteration_name,child_dataset_path.rsplit('/', 2)[2])\n",
    "\n",
    "                #Link child data to master file\n",
    "                hdf5_file_all_MC_results[master_dataset_path] = h5py.ExternalLink(child_file_path, child_dataset_path)\n",
    "\n",
    "        complete_iterations=complete_iterations+int(child_hdf5_file.attrs['Number of complete iterations'])\n",
    "        child_hdf5_file.close()\n",
    "        \n",
    "        end_2 = time.time()\n",
    "        print(\"Gathering information in {} secondes for entire DB for {} iterations\".format(end_2 - start_2,complete_iterations)) \n",
    "\n",
    "        \n",
    "    hdf5_file_all_MC_results.attrs['Number of complete iterations']=complete_iterations\n",
    "    \n",
    "    #Useful info\n",
    "    for child_file_path in child_DB_info_paths:\n",
    "        \n",
    "        child_hdf5_file=h5py.File(child_file_path,'r')\n",
    "    \n",
    "        for (child_dataset_path, dset) in h5py_dataset_iterator(child_hdf5_file):\n",
    "            \n",
    "            #Create the master path\n",
    "            master_dataset_path=child_dataset_path\n",
    "            \n",
    "            #Link child data to master file\n",
    "            hdf5_file_all_MC_results[master_dataset_path] = h5py.ExternalLink(child_file_path, child_dataset_path)\n",
    "        \n",
    "        db_name=child_hdf5_file.attrs['Database name']\n",
    "        child_hdf5_file.close()\n",
    "        \n",
    "    hdf5_file_all_MC_results.attrs['Database name']=db_name\n",
    "    \n",
    "    hdf5_file_all_MC_results.close()\n",
    "    \n",
    "        \n",
    "    return;\n",
    "   \n",
    "\n",
    "#######Functions to calculate correlated LCI######\n",
    "\n",
    "def calculate_ratio_corr_lognormal(gmean_Xi,gmean_Yj,gsigma_Xi,gsigma_Yj):\n",
    "    \n",
    "    #We assume that Xi and Yj are perfeclty correlated\n",
    "    gsigma_XiYj=gsigma_Xi*gsigma_Yj\n",
    "    \n",
    "    gmean_Aij=gmean_Xi-gmean_Yj\n",
    "    gsigma_Aij=np.sqrt((gsigma_Xi**2)+(gsigma_Yj**2)-(2*gsigma_XiYj))\n",
    "    \n",
    "    if np.isnan(gsigma_Aij) or gsigma_Aij==0:\n",
    "        gsigma_Aij=0.0000000000001\n",
    "    \n",
    "    return {'loc':gmean_Aij,'scale':gsigma_Aij,'uncertainty_type':2};\n",
    "\n",
    "\n",
    "#Only once for the DB\n",
    "def calculate_ratio_corr_lognormal_for_DB(corr_ef_code_input,\n",
    "                                          corr_ef_code_output,\n",
    "                                          corr_twin_ef_input_output,\n",
    "                                          biosphere_dict,\n",
    "                                          bio_params,biosphere_names):\n",
    "    \n",
    "    #corr_bio_params=bio_params\n",
    "    ratio_rv_dict_per_activity={}\n",
    "    inv_biosphere_dict={indice: ef for ef, indice in biosphere_dict.items()}\n",
    "    \n",
    "    set_efs=set([key[1] for key in biosphere_dict.keys()])\n",
    "    \n",
    "    #Isolate bio_params of interest: from corr_ef_code and with lognormal uncertainty\n",
    "    corr_input_indices=[biosphere_dict[('biosphere3',code)] for code in corr_ef_code_input if code in set_efs]\n",
    "    corr_output_indices=[biosphere_dict[('biosphere3',code)] for code in corr_ef_code_output if code in set_efs]\n",
    "    corr_twin_indices=[(biosphere_dict[('biosphere3',code[0])],biosphere_dict[('biosphere3',code[1])]) for code in corr_twin_ef_input_output if (code[0] in set_efs and code[1] in set_efs)]\n",
    "    corr_input_twin_indices=[indice[0] for indice in corr_twin_indices]\n",
    "    corr_output_twin_indices=[indice[1] for indice in corr_twin_indices]\n",
    "    corr_twin_indices_dict={indice[0]:indice[1] for indice in corr_twin_indices}\n",
    "    \n",
    "    #print(corr_twin_indices_dict)\n",
    "    \n",
    "    set_input=set(corr_input_indices)\n",
    "    set_output=set(corr_output_indices)\n",
    "    set_input_output=set(corr_input_indices+corr_output_indices)\n",
    "    \n",
    "    set_twin=set(corr_twin_indices)\n",
    "    set_input_twin=set(corr_input_twin_indices)\n",
    "    set_output_twin=set(corr_output_twin_indices)\n",
    "    set_input_output_twin=set(corr_input_twin_indices+corr_output_twin_indices)\n",
    "    \n",
    "    \n",
    "    selected_bio_params=np.array([bio_param for bio_param in bio_params if (bio_param['row'] in set_input_output and bio_param['uncertainty_type']==2)])\n",
    "    \n",
    "    #Per activity\n",
    "    set_col_indices=list(set(selected_bio_params['col']))\n",
    "    \n",
    "    for col in set_col_indices:\n",
    "        \n",
    "        act_bio_params=np.array([bio_param for bio_param in selected_bio_params if bio_param['col']==col])\n",
    "        \n",
    "        #Test if {input}=0 or {output}=0 --> exclude the activity from the list\n",
    "        act_input_params=np.array([bio_param for bio_param in act_bio_params if bio_param['row'] in set_input])\n",
    "        act_output_params=np.array([bio_param for bio_param in act_bio_params if bio_param['row'] in set_output])\n",
    "        \n",
    "        try:\n",
    "            act_input_params[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            act_output_params[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        #Test if input=output for twin ef --> remove uncertainty from the bio_params\n",
    "        #act_input_twin_params=np.array([bio_param for bio_param in act_input_params if (bio_param['row'] in set_input_twin and bio_param['row'] in set_input_output_twin)])\n",
    "        #act_output_twin_params=np.array([bio_param for bio_param in act_output_params if (bio_param['row'] in set_output_twin and bio_param['row'] in set_input_output_twin)])\n",
    "        #\n",
    "        #try:\n",
    "        #    act_input_twin_params[0]\n",
    "        #    \n",
    "        #    for twin_input_row in act_input_twin_params['row']:\n",
    "        #        twin_output_row=corr_twin_indices_dict[twin_input_row]\n",
    "        #        \n",
    "        #        output_twin_params=np.array([bio_param for bio_param in act_output_twin_params if bio_param['row']==twin_output_row])\n",
    "        #        \n",
    "        #        try:\n",
    "        #            output_twin_param=output_twin_params[0]\n",
    "        #            input_twin_param=np.array([bio_param for bio_param in act_input_twin_params if bio_param['row']==twin_input_row])[0]\n",
    "        #            \n",
    "        #            input_row_col_to_remove=(input_twin_param['row'],input_twin_param['col'])\n",
    "        #            output_row_col_to_remove=(output_twin_param['row'],output_twin_param['col'])\n",
    "        #                                \n",
    "        #            if input_twin_param['scale']==output_twin_param['scale']:                        \n",
    "        #                input_condition=np.where((corr_bio_params['row'] == input_row_col_to_remove[0]) * (corr_bio_params['col'] == input_row_col_to_remove[1]))\n",
    "        #                corr_bio_params[input_condition[0].tolist()[0]]['uncertainty_type']=1\n",
    "        #                act_input_params=[bio_param for bio_param in act_input_params if (bio_param['row'] != input_row_col_to_remove[0] and bio_param['col'] != input_row_col_to_remove[1])]\n",
    "        #                \n",
    "        #                output_condition=np.where((corr_bio_params['row'] == output_row_col_to_remove[0]) * (corr_bio_params['col'] == output_row_col_to_remove[1]))\n",
    "        #                corr_bio_params[output_condition[0].tolist()[0]]['uncertainty_type']=1\n",
    "        #                act_output_params=[bio_param for bio_param in act_output_params if (bio_param['row'] != output_row_col_to_remove[0] and bio_param['col'] != output_row_col_to_remove[1])]\n",
    "        #                \n",
    "        #                print(col,biosphere_names[inv_biosphere_dict[input_row_col_to_remove[0]]],biosphere_names[inv_biosphere_dict[output_row_col_to_remove[0]]])\n",
    "        #                \n",
    "        #        except IndexError:\n",
    "        #            pass\n",
    "        #    \n",
    "        #except IndexError:\n",
    "        #    pass\n",
    "            \n",
    "        \n",
    "        #Calculate Aij ratio lognormal rv and Store Aij and associate it with Yj only\n",
    "        for input_param in act_input_params:\n",
    "            for output_param in act_output_params:\n",
    "                gmean_Xi=input_param['loc']\n",
    "                gmean_Yj=output_param['loc']\n",
    "                gsigma_Xi=input_param['scale']\n",
    "                gsigma_Yj=output_param['scale']\n",
    "                params_Aij_dict=calculate_ratio_corr_lognormal(gmean_Xi,gmean_Yj,gsigma_Xi,gsigma_Yj)\n",
    "                ratio_rv_dict_per_activity[col]={output_param['row']:{input_param['row']:params_Aij_dict}}\n",
    "                \n",
    "                #print(type(params_Aij_dict['scale']))\n",
    "                \n",
    "                if np.isnan(params_Aij_dict['scale']):\n",
    "                    print(col,biosphere_names[inv_biosphere_dict[input_param['row']]],biosphere_names[inv_biosphere_dict[output_param['row']]])\n",
    "                                                 \n",
    "    return ratio_rv_dict_per_activity; #return (ratio_rv_dict_per_activity,corr_bio_params);\n",
    "\n",
    "#Each iteration after regenerating the B matrix\n",
    "def generate_corr_biosphere_matrix(ratio_rv_dict_per_activity,biosphere_dict, biosphere_matrix):\n",
    "    \n",
    "    corr_biosphere_matrix=biosphere_matrix.copy()\n",
    "    \n",
    "    #per activity\n",
    "    for col,A in ratio_rv_dict_per_activity.items():\n",
    "        \n",
    "        #per output Yj\n",
    "        for output_row,Aj in A.items():\n",
    "            \n",
    "            #print(Aj)\n",
    "\n",
    "            #Retrieve sum of estimation of Xi\n",
    "            input_rows=Aj.keys()\n",
    "            sum_xi=sum([biosphere_matrix[row,col] for row in input_rows])\n",
    "\n",
    "            #Generate Aij\n",
    "            Aij_rvs = stats_arrays.UncertaintyBase.from_dicts(*Aj.values())\n",
    "            Aij_rng = stats_arrays.MCRandomNumberGenerator(Aij_rvs)\n",
    "            aij=next(Aij_rng)\n",
    "            sum_aij=sum(aij)\n",
    "                \n",
    "            #Calculate Yj\n",
    "            yj=sum_xi/sum_aij\n",
    "\n",
    "            #Replace Yj values in the biosphere_matrix\n",
    "            corr_biosphere_matrix[output_row,col]=yj\n",
    "    \n",
    "    return corr_biosphere_matrix;\n",
    "\n",
    "#Dependant LCI Monte Carlo for each activity and functional unit defined in functional_units = [{act.key: FU}]\n",
    "def worker_process(project, job_id, worker_id, functional_units, iterations,path_for_saving,collector_functional_unit):\n",
    "    \n",
    "    #Open the HDF5 file for each worker\n",
    "    hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker{}.hdf5\".format(str(worker_id))\n",
    "    hdf5_file_MC_results_path=\"{}\\\\{}\".format(path_for_saving,hdf5_file_name)\n",
    "        \n",
    "    hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "    \n",
    "    #for LCI physical correlation: water and land transformation\n",
    "    projects.set_current(project)\n",
    "    biosphere3=Database('biosphere3')\n",
    "    \n",
    "    lca_1 = LCA(collector_functional_unit)\n",
    "    lca_1.lci()\n",
    "    \n",
    "    land_transfo_efs=[ef for ef in biosphere3 if 'Transformation, ' in str(ef)]\n",
    "    land_transfo_input={ef['name'].rsplit('Transformation, from ',1)[1]:ef for ef in land_transfo_efs if ', from' in str(ef)}\n",
    "    land_transfo_output={ef['name'].rsplit('Transformation, to ',1)[1]:ef for ef in land_transfo_efs if ', to' in str(ef)}\n",
    "    corr_twin_land_transfo=[(land_transfo_input[end_name]['code'],land_transfo_output[end_name]['code']) for end_name in land_transfo_input.keys()]\n",
    "    land_transfo_code_input=[ef['code'] for ef in land_transfo_input.values()]\n",
    "    land_transfo_code_output=[ef['code'] for ef in land_transfo_output.values()]\n",
    "    \n",
    "    water_efs=[ef for ef in biosphere3 if 'Water' in str(ef['name'])]\n",
    "    water_code_input=[ef['code'] for ef in water_efs if 'resource' in str(ef['categories'])]\n",
    "    water_code_output=[ef['code'] for ef in water_efs if 'resource' not in str(ef['categories'])]\n",
    "    corr_twin_water=[]\n",
    "    \n",
    "    biosphere_dict=lca_1.biosphere_dict\n",
    "    bio_params=lca_1.bio_params\n",
    "    biosphere_names={('biosphere3',ef['code']):ef['name'] for ef in biosphere3}    \n",
    "    \n",
    "    try:\n",
    "\n",
    "        #Retrieve the number of complete iterations (all activities calculated for one iteration)\n",
    "        try:\n",
    "            complete_iterations=int(hdf5_file_MC_results.attrs['Number of complete iterations'])\n",
    "            print('--Previous number of complete iterations for worker {} is {}'.format(worker_id, complete_iterations))\n",
    "        except:\n",
    "            hdf5_file_MC_results.attrs['Number of complete iterations']=0\n",
    "            complete_iterations=0\n",
    "\n",
    "        #Clean the HDF5 files for not complete iterations if needed\n",
    "        if complete_iterations>0:\n",
    "            clean_hdf5_file_MC_results(hdf5_file_MC_results,worker_id)\n",
    "\n",
    "        projects.set_current(project)\n",
    "\n",
    "        #Creating the LCA object --> set fix_dictionaries=False as not useful here?\n",
    "        lca = DirectSolvingMonteCarloLCA(demand = functional_units[0])\n",
    "        lca.load_data()\n",
    "        \n",
    "        #for LCI physical correlation: water and land transformation        \n",
    "        corr_ef_code_input=land_transfo_code_input\n",
    "        corr_ef_code_output=land_transfo_code_output\n",
    "        corr_twin_ef_input_output=corr_twin_land_transfo\n",
    "        ratio_rv_dict_per_activity_land_transfo=calculate_ratio_corr_lognormal_for_DB(corr_ef_code_input, \n",
    "                                      corr_ef_code_output, \n",
    "                                      corr_twin_ef_input_output, \n",
    "                                      biosphere_dict, \n",
    "                                      bio_params,biosphere_names)\n",
    "\n",
    "        corr_ef_code_input=water_code_input\n",
    "        corr_ef_code_output=water_code_output\n",
    "        corr_twin_ef_input_output=corr_twin_water\n",
    "        ratio_rv_dict_per_activity_water=calculate_ratio_corr_lognormal_for_DB(corr_ef_code_input, \n",
    "                                      corr_ef_code_output, \n",
    "                                      corr_twin_ef_input_output, \n",
    "                                      biosphere_dict, \n",
    "                                      bio_params,biosphere_names)\n",
    "        \n",
    "        \n",
    "        start_iteration = 0\n",
    "        end_iteration = 0\n",
    "\n",
    "        #Create and save objects per iteration --> Per iteration: A=0.49MB, B=0.34MB, creation time for both=0.35sec \n",
    "        for iteration in range(iterations):\n",
    "\n",
    "            #Name of the iteration for the storage, starts from 0\n",
    "            iteration_name=complete_iterations\n",
    "\n",
    "            print('--Starting job for worker {}, iteration {}, stored as {}, previous complete iteration in {} min'.format(worker_id, iteration,iteration_name,(end_iteration-start_iteration)/60))\n",
    "\n",
    "            start_iteration = time.time()\n",
    "\n",
    "            #start_1 = time.time()\n",
    "            #Creating A and B matrix\n",
    "            lca.rebuild_technosphere_matrix(lca.tech_rng.next())\n",
    "            lca.rebuild_biosphere_matrix(lca.bio_rng.next())\n",
    "            \n",
    "            #LCI physical correlation\n",
    "            biosphere_matrix=lca.biosphere_matrix\n",
    "            lca.biosphere_matrix=generate_corr_biosphere_matrix(ratio_rv_dict_per_activity_land_transfo,biosphere_dict, biosphere_matrix)\n",
    "            biosphere_matrix=lca.biosphere_matrix\n",
    "            lca.biosphere_matrix=generate_corr_biosphere_matrix(ratio_rv_dict_per_activity_water,biosphere_dict, biosphere_matrix)\n",
    "            \n",
    "            #Saving A and B to HDF5 file\n",
    "            group_path_techno='/technosphere_matrix/{}'.format(str(iteration_name))\n",
    "            group_path_bio='/biosphere_matrix/{}'.format(str(iteration_name))\n",
    "            write_LCA_obj_to_HDF5_file(lca.technosphere_matrix,hdf5_file_MC_results,group_path_techno)\n",
    "            write_LCA_obj_to_HDF5_file(lca.biosphere_matrix,hdf5_file_MC_results,group_path_bio)\n",
    "            hdf5_file_MC_results[group_path_techno].attrs['Creation ID']=job_id\n",
    "            hdf5_file_MC_results[group_path_bio].attrs['Creation ID']=job_id\n",
    "\n",
    "            #Size_A_MB=(hdf5_file_MC_results[group_path_techno+'/data'].id.get_storage_size()+hdf5_file_MC_results[group_path_techno+'/indptr'].id.get_storage_size()+hdf5_file_MC_results[group_path_techno+'/indices'].id.get_storage_size())/1000000\n",
    "            #Size_B_MB=(hdf5_file_MC_results[group_path_bio+'/data'].id.get_storage_size()+hdf5_file_MC_results[group_path_bio+'/indptr'].id.get_storage_size()+hdf5_file_MC_results[group_path_bio+'/indices'].id.get_storage_size())/1000000\n",
    "\n",
    "\n",
    "\n",
    "            #For calculation\n",
    "            lca.decompose_technosphere()\n",
    "\n",
    "            #end_1 = time.time()\n",
    "            #print(\"Calcul et sauvegarde A et B en {} secondes for iteration {}, and the storage size is A={}MB and B={}MB\".format(end_1 - start_1,iteration,Size_A_MB,Size_B_MB)) \n",
    "\n",
    "            #Create and save objects per activity/iteration --> Per iteration and activity: supply_array=0.04MB, creation time=0.01sec (except for the first activity=0.5sec)\n",
    "            for act_index, fu in enumerate(functional_units):\n",
    "\n",
    "                #start_2 = time.time()\n",
    "\n",
    "                #Creating UUID for each activity\n",
    "                actKey = list(fu.keys())[0][1]\n",
    "\n",
    "                #Create demand_array\n",
    "                lca.build_demand_array(fu)\n",
    "\n",
    "                #Create supply_array\n",
    "                lca.supply_array = lca.solve_linear_system()\n",
    "\n",
    "                #Save supply_array to HDF5 file\n",
    "                group_path_supply='/supply_array/{}/{}'.format(actKey,str(iteration_name))\n",
    "                write_LCA_obj_to_HDF5_file(lca.supply_array,hdf5_file_MC_results,group_path_supply)\n",
    "                hdf5_file_MC_results[group_path_supply].attrs['Creation ID']=job_id\n",
    "\n",
    "                #end_2 = time.time()\n",
    "                #print(\"Calcul et sauvegarde s en {} secondes for iteration {} and activity {}, and the storage size is s={}MB\".format(end_2 - start_2,iteration,actKey,hdf5_file_MC_results[group_path_supply].id.get_storage_size()/1000000)) \n",
    "\n",
    "            #Count the number of complete iterations\n",
    "            complete_iterations=iteration_name+1\n",
    "            hdf5_file_MC_results.attrs['Number of complete iterations']= complete_iterations\n",
    "\n",
    "            end_iteration = time.time()\n",
    "\n",
    "\n",
    "        hdf5_file_MC_results.close()\n",
    "        \n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        \n",
    "        hdf5_file_MC_results.flush()\n",
    "        hdf5_file_MC_results.close()\n",
    "        \n",
    "        print(\"Process interrupted\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    return;\n",
    "\n",
    "#TEST OK    \n",
    "def get_useful_info(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities):\n",
    "\n",
    "    # Sacrificial LCA to extract relevant information (demand of 1 for all activities)\n",
    "    # Done on the \"collector\" functional unit to ensure that all activities and \n",
    "    # exchanges are covered in the common dicts (only relevant if some activities \n",
    "    # link to other upstream databases\n",
    "    sacrificial_lca = LCA(collector_functional_unit)\n",
    "    sacrificial_lca.lci()\n",
    "    \n",
    "    #Get data to store \n",
    "    product_dict=sacrificial_lca.product_dict\n",
    "    _product_dict=sacrificial_lca._product_dict\n",
    "    biosphere_dict=sacrificial_lca.biosphere_dict\n",
    "    _biosphere_dict=sacrificial_lca._biosphere_dict\n",
    "    activity_dict=sacrificial_lca.activity_dict\n",
    "    _activity_dict=sacrificial_lca._activity_dict\n",
    "    tech_params=sacrificial_lca.tech_params\n",
    "    bio_params=sacrificial_lca.bio_params\n",
    "    activities_keys=np.string_([act.key[1] for act in activities])\n",
    "    #rev_activity_dict, rev_product_dict, rev_bio_dict = sacrificial_lca.reverse_dict()  --> really needed?\n",
    "    \n",
    "    #Get metadata for the HDF5 file\n",
    "    DB_name=activities[0].key[0]\n",
    "    \n",
    "    #Write useful information to HDF5 file\n",
    "    hdf5_file_useful_info_per_DB.attrs['Database name']=DB_name\n",
    "    hdf5_file_useful_info_per_DB.attrs['Creation ID']=job_id\n",
    "    hdf5_file_useful_info_per_DB.attrs['Description']='HDF5 file containing all useful information related to the database in order to use dependant Monte Carlo results'\n",
    "    \n",
    "    write_LCA_obj_to_HDF5_file(product_dict,hdf5_file_useful_info_per_DB,'/product_dict')\n",
    "    write_LCA_obj_to_HDF5_file(_product_dict,hdf5_file_useful_info_per_DB,'/_product_dict')\n",
    "    write_LCA_obj_to_HDF5_file(biosphere_dict,hdf5_file_useful_info_per_DB,'/biosphere_dict')\n",
    "    write_LCA_obj_to_HDF5_file(_biosphere_dict,hdf5_file_useful_info_per_DB,'/_biosphere_dict')\n",
    "    write_LCA_obj_to_HDF5_file(activity_dict,hdf5_file_useful_info_per_DB,'/activity_dict')\n",
    "    write_LCA_obj_to_HDF5_file(_activity_dict,hdf5_file_useful_info_per_DB,'/_activity_dict')\n",
    "    write_LCA_obj_to_HDF5_file(tech_params,hdf5_file_useful_info_per_DB,'/tech_params')\n",
    "    write_LCA_obj_to_HDF5_file(bio_params,hdf5_file_useful_info_per_DB,'/bio_params')\n",
    "    write_LCA_obj_to_HDF5_file(activities_keys,hdf5_file_useful_info_per_DB,'/activities_keys')\n",
    "    #write_LCA_obj_to_HDF5_file(rev_activity_dict,hdf5_file_useful_info_per_DB,'/rev_activity_dict')\n",
    "    #write_LCA_obj_to_HDF5_file(rev_product_dict,hdf5_file_useful_info_per_DB,'/rev_product_dict')\n",
    "    #write_LCA_obj_to_HDF5_file(rev_bio_dict,hdf5_file_useful_info_per_DB,'/rev_bio_dict')\n",
    "\n",
    "    return None;\n",
    "\n",
    "\n",
    "#Useful when the code is run from the console to pass arguments to the main function\n",
    "#@click.command()\n",
    "#@click.option('--project', default='default', help='Brightway2 project name', type=str)\n",
    "#@click.option('--database', help='Database name', type=str)\n",
    "#@click.option('--iterations', default=1000, help='Number of Monte Carlo iterations', type=int)\n",
    "#@click.option('--cpus', default=mp.cpu_count(), help='Number of used CPU cores', type=int)\n",
    "#@click.option('--output_dir', help='Output directory path', type=str)\n",
    "\n",
    "\n",
    "\n",
    "#Create and save useful information during Dependant LCI MC : database objects (_dict, activities, _params, reverse_dict), iteration objects (_sample, i.e. A and B _matrix), act/iteration objects (supply_array)    \n",
    "def Dependant_LCI_Monte_Carlo_results(project, database, iterations, cpus, activity_category_ID, output_dir):\n",
    "    \n",
    "    projects.set_current(project)\n",
    "    bw2setup()\n",
    "\n",
    "    #Path the write the results\n",
    "    BASE_OUTPUT_DIR = output_dir\n",
    "\n",
    "    #ID to identify who and when was the calculation made\n",
    "    now = datetime.datetime.now()\n",
    "    job_id = \"{}_{}-{}-{}_{}h{}\".format(os.environ['COMPUTERNAME'],now.year, now.month, now.day, now.hour, now.minute)\n",
    "\n",
    "    #Selection of activities for MC analysis\n",
    "    db = Database(database)\n",
    "    activities = [activity for activity in db if activity_category_ID in str(activity['classifications'])]\n",
    "    #act1=db.get('e929619f245df590fee5d72dc979cdd4')\n",
    "    #act2=db.get('bdf7116059abfcc6b8b9ade1a641e578')\n",
    "    #act3=db.get('c8c815c68836adaf964daaa001a638a3')\n",
    "    #activities = [act1,act2,act3]\n",
    "\n",
    "    #Create objects to pass the functional units = 1 for each activity\n",
    "    functional_units = [ {act.key: 1} for act in activities ]\n",
    "    collector_functional_unit = {k:v for d in functional_units for k, v in d.items()}\n",
    "\n",
    "    #Create or open the HDF5 file for useful information storage per DB\n",
    "    path_for_saving=BASE_OUTPUT_DIR\n",
    "    hdf5_file_name=\"Useful_info_per_DB.hdf5\"\n",
    "    hdf5_file_useful_info_per_DB_path=\"{}\\\\{}\".format(path_for_saving,hdf5_file_name)\n",
    "    \n",
    "    if os.path.isfile(hdf5_file_useful_info_per_DB_path)==False:\n",
    "\n",
    "        hdf5_file_useful_info_per_DB=h5py.File(hdf5_file_useful_info_per_DB_path,'a')\n",
    "\n",
    "        #Create and save all the useful information related to the database only\n",
    "        get_useful_info(collector_functional_unit, hdf5_file_useful_info_per_DB, job_id, activities)\n",
    "\n",
    "        hdf5_file_useful_info_per_DB.close()\n",
    "\n",
    "    #Code to slipt the work between each CPUs of the computer (called workers). The work refers here to the dependant LCI MC for each activity \n",
    "    workers = []\n",
    "\n",
    "    for worker_id in range(cpus):\n",
    "        #Create or open the HDF5 file for each worker and write metadata\n",
    "        hdf5_file_name=\"LCI_Dependant_Monte_Carlo_results_worker\"+str(worker_id)+\".hdf5\"\n",
    "        hdf5_file_MC_results_path=\"{}\\\\{}\".format(BASE_OUTPUT_DIR,hdf5_file_name)\n",
    "\n",
    "        hdf5_file_MC_results=h5py.File(hdf5_file_MC_results_path,'a')\n",
    "\n",
    "        hdf5_file_MC_results.attrs['Database name']=db.name\n",
    "        hdf5_file_MC_results.attrs['Worker ID']=worker_id\n",
    "        hdf5_file_MC_results.attrs['Description']='HDF5 file containing all dependant Monte Carlo results per iteration (A and B matrix) and per activity/iteration (supply_array)'\n",
    "\n",
    "        hdf5_file_MC_results.close()\n",
    "\n",
    "        # Create child processes that can work apart from parent process\n",
    "        child = mp.Process(target=worker_process, args=(projects.current, job_id, worker_id, functional_units, iterations,path_for_saving,collector_functional_unit))\n",
    "        workers.append(child)\n",
    "        child.start()\n",
    "        \n",
    "    return;\n",
    "      \n",
    "#Commande to launch it from the console : python database_wide_monte_carlo_hdf5_storage.py        \n",
    "        \n",
    "#Useful when the code is run from the console to execute the main function\n",
    "if __name__ == '__main__':\n",
    "    Dependant_LCI_Monte_Carlo_results(project=\"iw_integration\",\n",
    "                                      database=\"ecoinvent 3.3 cutoff\",\n",
    "                                      iterations=250,\n",
    "                                      cpus=4,\n",
    "                                      activity_category_ID=\"4922\",\n",
    "                                      output_dir=\"D:\\\\Dropbox (MAGI)\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Monte Carlo results_with correlation\\\\Dependant LCI Monte Carlo - sector 4922\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCA Monte Carlo\n",
    "\n",
    "To be run in console as Database_wide_Monte_Carlo_LCA_calculation_with_correlation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from brightway2 import *\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from scipy.sparse.linalg import factorized, spsolve\n",
    "from scipy import sparse\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import click\n",
    "import scipy as sp\n",
    "import h5py\n",
    "from bw2calc.matrices import MatrixBuilder\n",
    "from stats_arrays.random import MCRandomNumberGenerator\n",
    "import Add_beta_4_params_distrib\n",
    "import random\n",
    "#import stats_arrays\n",
    "\n",
    "##################\n",
    "# HDF5 functions #\n",
    "##################\n",
    "\n",
    "# All those functions work based on LCA objects from Brightway\n",
    "\n",
    "# function to rebuild csr matrix from hdf5 storage\n",
    "\n",
    "def hdf5_to_csr_matrix(hdf5_file,group_full_path):\n",
    "    \n",
    "    # Access hdf5 group of the csr info\n",
    "    group=hdf5_file[group_full_path]\n",
    "    \n",
    "    #Rebuild csr matrix\n",
    "    csr=sp.sparse.csr_matrix((group['data'][:],group['indices'][:],group['indptr'][:]), group['shape'][:])\n",
    "    \n",
    "    return csr;\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _dict: biosphere_dict, activity_dict, product_dict\n",
    "def LCA_dict_to_hdf5(LCA_dict,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    ###### WARNING : Modify the builder because _dict items are like \n",
    "    #####('ecoinvent 3.3 cutoff', 'c533b046462b6c56a5636ca177347c48'): 35\n",
    "    #### Use .decode('UTF-8') to convert keys_1_list items for bytes to str\n",
    "    \n",
    "    \n",
    "    keys_0=np.string_([key[0] for key in LCA_dict.keys()][0])\n",
    "    keys_1_list=np.string_([key[1] for key in LCA_dict.keys()])\n",
    "    items_list=[item for item in LCA_dict.values()]\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('keys_1',data=keys_1_list,compression=\"gzip\")\n",
    "    group.create_dataset('values',data=items_list,compression=\"gzip\")\n",
    "    group.create_dataset('keys_0',data=keys_0)\n",
    "\n",
    "    return;\n",
    "\n",
    "#function to rebuild LCA object _dict: biosphere_dict, activity_dict, product_dict\n",
    "def hdf5_to_LCA_dict(hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)\n",
    "    \n",
    "    #Retrieve Keys and Items\n",
    "    keys_0=group['keys_0'][()].decode('UTF-8')\n",
    "    keys_1_list=[key_1.decode('UTF-8') for key_1 in group['keys_1'][()]] #### Use .decode('UTF-8') to convert keys_1_list items for bytes to str ?\n",
    "    items_list=group['values'][()]\n",
    "    \n",
    "    keys_list=[(keys_0,keys_1) for keys_1 in keys_1_list]\n",
    "    \n",
    "    #Rebuild LCA_dict\n",
    "    LCA_dict={}\n",
    "    \n",
    "    LCA_dict=dict(zip(keys_list,items_list))\n",
    "    \n",
    "    return LCA_dict;\n",
    "\n",
    "\n",
    "#function to create list to store in hfd5 for LCA object _***_dict: _biosphere_dict, _activity_dict, _product_dict\n",
    "def _LCA_dict_to_hdf5(LCA_dict,hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)  \n",
    "    \n",
    "    keys=[key for key in LCA_dict.keys()]\n",
    "    values=[item for item in LCA_dict.values()]\n",
    "    \n",
    "    # Create datasets containing values of csr matrix\n",
    "    group.create_dataset('keys',data=keys,compression=\"gzip\")\n",
    "    group.create_dataset('values',data=values,compression=\"gzip\")\n",
    "\n",
    "    return;\n",
    "\n",
    "#function to rebuild LCA object _***_dict: _biosphere_dict, _activity_dict, _product_dict\n",
    "def _hdf5_to_LCA_dict(hdf5_file,group_path):\n",
    "    \n",
    "    # Retrieve or create the groups and subgroups\n",
    "    group=hdf5_file.require_group(group_path)  \n",
    "    \n",
    "    keys=group['keys'][()]\n",
    "    values=group['values'][()]\n",
    "    \n",
    "    #Rebuild LCA_dict\n",
    "    LCA_dict={}\n",
    "    \n",
    "    LCA_dict=dict(zip(keys,values))\n",
    "\n",
    "    return LCA_dict;\n",
    "\n",
    "\n",
    "#Function to write csr matrix, _dict from LCA objects and any numpy.ndarray\n",
    "\n",
    "def write_LCA_obj_to_HDF5_file(LCA_obj,hdf5_file,group_path):\n",
    "    \n",
    "    dict_names_to_check=['biosphere_dict', 'activity_dict', 'product_dict']\n",
    "    _dict_names_to_check=['_biosphere_dict', '_activity_dict', '_product_dict']\n",
    "    \n",
    "    #If object = A or B matrix\n",
    "    if type(LCA_obj)==sp.sparse.csr.csr_matrix:\n",
    "        #csr_matrix_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "        #### Direct copy of the function because the call to the function does not work... --> Works now!\n",
    "        # Retrieve or create groups and subgroups\n",
    "        group=hdf5_file.require_group(group_path)\n",
    "\n",
    "        # Create datasets containing values of csr matrix\n",
    "        group.create_dataset('data',data=LCA_obj.data,compression=\"gzip\",dtype=np.float32)\n",
    "        group.create_dataset('indptr',data=LCA_obj.indptr,compression=\"gzip\")\n",
    "        group.create_dataset('indices',data=LCA_obj.indices,compression=\"gzip\")\n",
    "        group.create_dataset('shape',data=LCA_obj.shape,compression=\"gzip\")\n",
    "        ######\n",
    "        \n",
    "    #If object = _***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in _dict_names_to_check:\n",
    "        _LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)    \n",
    "            \n",
    "    \n",
    "    #If object = ***_dict\n",
    "    elif group_path.rsplit('/', 1)[1] in dict_names_to_check:\n",
    "        LCA_dict_to_hdf5(LCA_obj,hdf5_file,group_path)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #store as float32 if type is float64 to save space\n",
    "        if LCA_obj.dtype == np.dtype('float64'):\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\",dtype=np.float32)\n",
    "            \n",
    "        else:\n",
    "            hdf5_file.create_dataset(group_path,data=LCA_obj,compression=\"gzip\")\n",
    "            \n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "def h5py_dataset_iterator(g, prefix=''):\n",
    "    for key in g.keys():\n",
    "        item = g[key]\n",
    "        path = '{}/{}'.format(prefix, key)\n",
    "        if isinstance(item, h5py.Dataset): # test for dataset\n",
    "            yield (path, item)\n",
    "        elif isinstance(item, h5py.Group): # test for group (go down)\n",
    "            yield from h5py_dataset_iterator(item, path)\n",
    "\n",
    "            \n",
    "#Create a dataset and append an np array to the dataset 1d in hdf5            \n",
    "def append_to_list_hdf5_dataset(hdf5_file,dataset_path,value):\n",
    "    \n",
    "    #value is a scalar or a list or np array\n",
    "    \n",
    "    try:\n",
    "        dataset=hdf5_file[dataset_path]\n",
    "        dataset.resize((dataset.shape[0]+len(value),))\n",
    "        dataset[-len(value):] = value\n",
    "    except:\n",
    "        hdf5_file.create_dataset(dataset_path,shape=(len(value),), maxshape=(None,))\n",
    "        dataset=hdf5_file[dataset_path]\n",
    "        dataset[-len(value):] = value\n",
    "        \n",
    "    return;\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Dependant LCI Monte Carlo functions #\n",
    "#######################################\n",
    "\n",
    "\n",
    "#Clean the HDF5 file with LCA MC results for not complete iterations (i.e. not all activities calculated for the last iteration)\n",
    "def clean_hdf5_file_MC_LCA_results(hdf5_file_MC_results,worker_id):\n",
    "    \n",
    "    disaggregated_root_file_name='LCA_Dependant_Monte_Carlo_disaggregated_results'\n",
    "    aggregated_root_file_name='LCA_Dependant_Monte_Carlo_aggregated_results'\n",
    "    \n",
    "    complete_iterations=int(hdf5_file_MC_results.attrs['Number of complete iterations'])\n",
    "    \n",
    "    if aggregated_root_file_name in hdf5_file_MC_results.filename:\n",
    "        for (dataset_path, dset) in h5py_dataset_iterator(hdf5_file_MC_results):\n",
    "            if '/lci_iteration_name_list' not in dataset_path:\n",
    "                results_iterations=dset.size\n",
    "                if results_iterations!=complete_iterations:\n",
    "                    dataset.resize((dataset.shape[0]-1,))\n",
    "                    print('--Incomplete iterations for LCA results removed for worker {} for {}'.format(worker_id,dataset_path))\n",
    "                    \n",
    "                \n",
    "    elif disaggregated_root_file_name in hdf5_file_MC_results.filename:\n",
    "        for uncertainty_level in hdf5_file_MC_results.items():\n",
    "            if 'lci_iteration_name_list' not in uncertainty_level[0]:\n",
    "                for act in uncertainty_level[1].items():\n",
    "                    for impact_method in act[1].items():\n",
    "                        results_iterations=len(impact_method[1])\n",
    "                        if results_iterations!=complete_iterations:\n",
    "                            iteration_name_to_delete=results_iterations-1\n",
    "                            del impact_method[1][str(iteration_name_to_delete)]\n",
    "                            #print('--Incomplete iterations for LCA results removed for worker {} for activity {} and impact method'.format(worker_id,act[0],impact_method[0]))\n",
    "                            \n",
    "    return;\n",
    "\n",
    "\n",
    "#Create a file that gather all MC results and Useful info in one file --> Copy of the results for aggregated results!\n",
    "def gathering_MC_results_in_one_hdf5_file(path_for_saving):\n",
    "    \n",
    "    disaggregated_root_file_name='LCA_Dependant_Monte_Carlo_disaggregated_results'\n",
    "    aggregated_root_file_name='LCA_Dependant_Monte_Carlo_aggregated_results'\n",
    "    \n",
    "    root_file_name_list=[aggregated_root_file_name,disaggregated_root_file_name]\n",
    "    \n",
    "    for root_file_name in root_file_name_list:\n",
    "\n",
    "        #Retrieve child file paths\n",
    "        child_hdf5_file_paths = [os.path.join(path_for_saving,fn) for fn in next(os.walk(path_for_saving))[2] if '.hdf5' in fn]\n",
    "        child_MC_results_paths=[path for path in child_hdf5_file_paths if '{}_worker'.format(root_file_name) in path]\n",
    "\n",
    "        if child_MC_results_paths!=[]:\n",
    "        \n",
    "            #Create the gathering file\n",
    "            hdf5_file_all_MC_results_path=os.path.join(path_for_saving,root_file_name,'_ALL.hdf5')\n",
    "            hdf5_file_all_MC_results=h5py.File(hdf5_file_all_MC_results_path,'w-')\n",
    "\n",
    "            #Gathering MC results\n",
    "            complete_iterations=0\n",
    "\n",
    "            for child_file_path in child_MC_results_paths:\n",
    "\n",
    "                #Clean incomplete iterations before gathering \n",
    "                child_hdf5_file=h5py.File(child_file_path,'a')\n",
    "                clean_hdf5_file_MC_LCA_results(child_hdf5_file,child_file_path.rsplit('_worker', 1)[1])\n",
    "                child_hdf5_file.close()\n",
    "\n",
    "                child_hdf5_file=h5py.File(child_file_path,'r')\n",
    "                \n",
    "                if disaggregated_root_file_name in child_file_path:\n",
    "\n",
    "                    for (child_dataset_path, dset) in h5py_dataset_iterator(child_hdf5_file):\n",
    "                        \n",
    "                        if 'lci_iteration_name_list' in child_dataset_path:\n",
    "                            master_dataset_path=child_dataset_path\n",
    "                            append_to_list_hdf5_dataset(hdf5_file_all_MC_results,master_dataset_path,dset[()])\n",
    "                            \n",
    "                        else:\n",
    "                            #Create similar path for the master file\n",
    "                            child_iteration_name=child_dataset_path.rsplit('/', 2)[1]\n",
    "                            master_iteration_name=str(int(child_iteration_name)+complete_iterations)\n",
    "                            master_dataset_path='{}/{}/{}'.format(child_dataset_path.rsplit('/', 2)[0],master_iteration_name,child_dataset_path.rsplit('/', 2)[2])\n",
    "\n",
    "                            #Link child data to master file\n",
    "                            hdf5_file_all_MC_results[master_dataset_path] = h5py.ExternalLink(child_file_path, child_dataset_path)\n",
    "\n",
    "                        \n",
    "                elif aggregated_root_file_name in child_file_path:\n",
    "\n",
    "                    for (child_dataset_path, dset) in h5py_dataset_iterator(child_hdf5_file):       \n",
    "                        \n",
    "                        #Not external links --> Copy of the results!\n",
    "                        master_dataset_path=child_dataset_path\n",
    "                        append_to_list_hdf5_dataset(hdf5_file_all_MC_results,master_dataset_path,dset[()])\n",
    "                        \n",
    "                complete_iterations=complete_iterations+int(child_hdf5_file.attrs['Number of complete iterations'])\n",
    "                db_name=child_hdf5_file.attrs['Database name']\n",
    "                child_hdf5_file.close()\n",
    "                \n",
    "            hdf5_file_all_MC_results.attrs['Number of complete iterations']=complete_iterations\n",
    "            hdf5_file_all_MC_results.attrs['Database name']=db_name\n",
    "\n",
    "            hdf5_file_all_MC_results.close()\n",
    "                           \n",
    "        \n",
    "    return;\n",
    "    \n",
    "\n",
    "#LCIA correlation\n",
    "def generate_corr_CF(corr_twin_ef_input_output,biosphere_dict,cf_params,cf_values):\n",
    "    \n",
    "    cf_dict={cf_param['row']:cf_value for cf_param,cf_value in zip(cf_params,cf_values)}\n",
    "    \n",
    "    set_efs=set([key[1] for key in biosphere_dict.keys()])    \n",
    "    corr_twin_indices_dict={biosphere_dict[('biosphere3',code[0])]:biosphere_dict[('biosphere3',code[1])] for code in corr_twin_ef_input_output if (code[0] in set_efs and code[1] in set_efs)}\n",
    "    \n",
    "    for row,cf_value in cf_dict.items():\n",
    "        \n",
    "        try:\n",
    "            row_output=corr_twin_indices_dict[row]\n",
    "            cf_dict[row_output]=cf_value*(-1)\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    cf_values=np.array([value for value in cf_dict.values()])\n",
    "    \n",
    "    return cf_values;\n",
    "\n",
    "\n",
    "\n",
    "#Dependant LCA Monte Carlo for each activity and functional unit defined in functional_units = [{act.key: 1}]\n",
    "def worker_process(project, \n",
    "                   job_id, \n",
    "                   worker_id, \n",
    "                   iterations,\n",
    "                   functional_units,\n",
    "                   collector_functional_unit,\n",
    "                   hdf5_file_MC_LCI_results_path,\n",
    "                   hdf5_file_deterministic_lci_path_agg,\n",
    "                   hdf5_file_deterministic_lci_path_disagg,\n",
    "                   hdf5_file_MC_LCA_results_aggregated_path,\n",
    "                   hdf5_file_MC_LCA_results_disaggregated_path,\n",
    "                   impact_method_name_list,\n",
    "                   results_disaggregated_or_not):\n",
    "    \n",
    "    #start_1 = time.time()\n",
    "    \n",
    "    #import Add_beta_4_params_distrib\n",
    "    \n",
    "    projects.set_current(project)\n",
    "    \n",
    "    #for LCIA correlation: land transformation\n",
    "    biosphere3=Database('biosphere3')\n",
    "    land_transfo_efs=[ef for ef in biosphere3 if 'Transformation, ' in str(ef)]\n",
    "    land_transfo_input={ef['name'].rsplit('Transformation, from ',1)[1]:ef for ef in land_transfo_efs if ', from' in str(ef)}\n",
    "    land_transfo_output={ef['name'].rsplit('Transformation, to ',1)[1]:ef for ef in land_transfo_efs if ', to' in str(ef)}\n",
    "    corr_twin_land_transfo=[(land_transfo_input[end_name]['code'],land_transfo_output[end_name]['code']) for end_name in land_transfo_input.keys()]\n",
    "    \n",
    "    lca_1 = LCA(functional_units[0])\n",
    "    lca_1.lci()\n",
    "\n",
    "    \n",
    "    #Open the HDF5 files for each worker to write LCA results\n",
    "    hdf5_file_MC_LCI_results=h5py.File(hdf5_file_MC_LCI_results_path,'r')\n",
    "    lci_complete_iterations=int(hdf5_file_MC_LCI_results.attrs['Number of complete iterations'])\n",
    "    \n",
    "    if results_disaggregated_or_not == \"disaggregated\":\n",
    "        hdf5_file_MC_LCA_results=h5py.File(hdf5_file_MC_LCA_results_disaggregated_path,'a')\n",
    "    else:\n",
    "        hdf5_file_MC_LCA_results=h5py.File(hdf5_file_MC_LCA_results_aggregated_path,'a')\n",
    "    \n",
    "    if results_disaggregated_or_not == \"disaggregated\":\n",
    "        hdf5_file_deterministic_lci=h5py.File(hdf5_file_deterministic_lci_path_disagg,'r')\n",
    "    else:\n",
    "        hdf5_file_deterministic_lci=h5py.File(hdf5_file_deterministic_lci_path_agg,'r')\n",
    "    \n",
    "    #Retrieve or set the number of LCA complete iterations (all activities calculated for one iteration)\n",
    "    try:\n",
    "        lca_complete_iterations=int(hdf5_file_MC_LCA_results.attrs['Number of complete iterations'])\n",
    "        print('--Previous number of complete iterations for worker {} is {}'.format(worker_id, lca_complete_iterations))\n",
    "    except:\n",
    "        hdf5_file_MC_LCA_results.attrs['Number of complete iterations']=0\n",
    "        lca_complete_iterations=0\n",
    "        \n",
    "    \n",
    "    #Clean incomplete iterations if needed\n",
    "    if lca_complete_iterations>0:\n",
    "        clean_hdf5_file_MC_LCA_results(hdf5_file_MC_LCA_results,worker_id)\n",
    "    \n",
    "    \n",
    "    #Retrieve biosphere_dict and activity_dict\n",
    "    _biosphere_dict=_hdf5_to_LCA_dict(hdf5_file_MC_LCI_results,'/_biosphere_dict')\n",
    "    \n",
    "    if results_disaggregated_or_not == \"disaggregated\":\n",
    "        activity_dict=hdf5_to_LCA_dict(hdf5_file_MC_LCI_results,'/activity_dict')\n",
    "    \n",
    "    \n",
    "    #Construct the characterization useful info (cf_params and cf_rng) for all LCIA methods\n",
    "    impact_method_dict={}\n",
    "    \n",
    "    \n",
    "    for impact_method_name in impact_method_name_list:\n",
    "\n",
    "        print(impact_method_name)\n",
    "        \n",
    "        method_filepath = [Method(impact_method_name).filepath_processed()]\n",
    "\n",
    "        cf_params, _, _, characterization_matrix = MatrixBuilder.build(\n",
    "                    method_filepath,\n",
    "                    \"amount\",\n",
    "                    \"flow\",\n",
    "                    \"row\",\n",
    "                    row_dict=_biosphere_dict,\n",
    "                    one_d=True,\n",
    "        )\n",
    "\n",
    "        cf_rng = MCRandomNumberGenerator(cf_params, seed=None)\n",
    "        \n",
    "        impact_method_dict[impact_method_name]={}\n",
    "        impact_method_dict[impact_method_name]['cf_params']=cf_params\n",
    "        impact_method_dict[impact_method_name]['cf_rng']=cf_rng\n",
    "        impact_method_dict[impact_method_name]['characterization_matrix_deterministic']=characterization_matrix\n",
    "    \n",
    "   \n",
    "    #end_1 = time.time()\n",
    "    #print(\"Work per worker en {} secondes \".format(end_1 - start_1))\n",
    "    \n",
    "    #LCA iterations\n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        #start_2 = time.time()\n",
    "        start_iteration = time.time()\n",
    "        \n",
    "        #Name of the iteration for the storage, starts from 0\n",
    "        lca_iteration_name=lca_complete_iterations\n",
    "        \n",
    "        print('--Starting job for worker {}, iteration {}, stored as {}'.format(worker_id, iteration,lca_iteration_name))\n",
    "        \n",
    "        #Randomly choose an LCI iteration\n",
    "        lci_iteration_name=random.randint(0,lci_complete_iterations-1)\n",
    "        \n",
    "        #Retrieve B matrix for uncertainty LCI 1\n",
    "        biosphere_matrix=hdf5_to_csr_matrix(hdf5_file_MC_LCI_results,'/biosphere_matrix/{}'.format(str(lci_iteration_name)))      \n",
    "        \n",
    "        #Regenerate the characterization_matrix for each iteration for all impact methods\n",
    "        characterization_matrix_dict={}\n",
    "        \n",
    "        for impact_method_name in impact_method_dict:\n",
    "            \n",
    "            cf_params=impact_method_dict[impact_method_name]['cf_params']\n",
    "            cf_rng=impact_method_dict[impact_method_name]['cf_rng']\n",
    "            cf_values=cf_rng.next()\n",
    "            \n",
    "            #LCIA correlation: land transformation\n",
    "            corr_twin_ef_input_output=corr_twin_land_transfo\n",
    "            biosphere_dict=lca_1.biosphere_dict\n",
    "\n",
    "            cf_values=generate_corr_CF(corr_twin_ef_input_output,biosphere_dict,cf_params,cf_values)\n",
    "            \n",
    "            \n",
    "            characterization_matrix = MatrixBuilder.build_diagonal_matrix(cf_params, _biosphere_dict,\"row\", \"row\", new_data=cf_values)#For disaggregated results\n",
    "            \n",
    "            #For disaggregated results\n",
    "            if results_disaggregated_or_not == \"disaggregated\":\n",
    "                characterization_matrix_dict[impact_method_name]=characterization_matrix\n",
    "            \n",
    "            #For aggregated results\n",
    "            else:\n",
    "                characterization_matrix_array=np.array(characterization_matrix.sum(1))\n",
    "                characterization_matrix_dict[impact_method_name]=characterization_matrix_array\n",
    "            \n",
    "        #end_2 = time.time()\n",
    "        #print(\"Work per iteration en {} secondes for iteration {} \".format(end_2 - start_2, iteration))    \n",
    "            \n",
    "        #Iterations per activity\n",
    "        for act_index, fu in enumerate(functional_units):\n",
    "            \n",
    "            #start_3 = time.time()\n",
    "\n",
    "            #Creating UUID for each activity\n",
    "            actKey = list(fu.keys())[0][1]\n",
    "            \n",
    "            \n",
    "            #Retrieve the inventory for uncertainty LCI 0\n",
    "            if results_disaggregated_or_not == \"disaggregated\":\n",
    "                inventory_lci0_path='/inventory/{}'.format(actKey)\n",
    "                inventory_lci0=hdf5_to_csr_matrix(hdf5_file_deterministic_lci,inventory_lci0_path)\n",
    "            else:\n",
    "                inventory_lci0_path='/inventory/{}'.format(actKey)\n",
    "                inventory_lci0=hdf5_file_deterministic_lci[inventory_lci0_path][()]\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Retrieve supply_array for uncertainty LCI 1\n",
    "            supply_array=hdf5_file_MC_LCI_results['/supply_array/{}/{}'.format(actKey,str(lci_iteration_name))][()]\n",
    "            \n",
    "            #Calculate inventory for uncertainty LCI 1 \n",
    "            #For disaggregated results --> inventory is a csr matrix\n",
    "            if results_disaggregated_or_not == \"disaggregated\":\n",
    "                count = len(activity_dict)\n",
    "                inventory = biosphere_matrix * sparse.spdiags([supply_array], [0], count, count) #For disaggregated results\n",
    "            \n",
    "            #For aggregated results --> inventory is a vector\n",
    "            else:\n",
    "                #inventory_array=np.array(inventory.sum(1)) #For aggregated results\n",
    "\n",
    "                inventory = biosphere_matrix * supply_array \n",
    "            \n",
    "            #end_3 = time.time()\n",
    "            #print(\"Work per activity/iteration en {} secondes for iteration {} for activity {} \".format(end_3 - start_3, iteration, actKey))    \n",
    "            \n",
    "            \n",
    "            #Calculate impact scores for all impact categories and Store impact_score\n",
    "            for impact_method_name in characterization_matrix_dict:\n",
    "                \n",
    "                #start_4 = time.time()\n",
    "                \n",
    "                characterization_matrix=characterization_matrix_dict[impact_method_name]\n",
    "                characterization_matrix_lcia0=impact_method_dict[impact_method_name]['characterization_matrix_deterministic']\n",
    "                \n",
    "                impact_score_path='/Uncertainty LCI 1 LCIA 1/{}/{}'.format(actKey,str(impact_method_name))\n",
    "                impact_score_lcia0_path='/Uncertainty LCI 1 LCIA 0/{}/{}'.format(actKey,str(impact_method_name))\n",
    "                impact_score_lci0_lcia1_path='/Uncertainty LCI 0 LCIA 1/{}/{}'.format(actKey,str(impact_method_name))\n",
    "                \n",
    "                #For disaggregated results --> impact_score is a csr matrix of impact scores \n",
    "                #disaggregated per direct contributing activity and direct contributing elementary flows\n",
    "                if results_disaggregated_or_not == \"disaggregated\":\n",
    "                    \n",
    "                    #Uncertainty LCI 1 LCIA 1\n",
    "                    impact_score= characterization_matrix * inventory\n",
    "                    impact_score_path='{}/{}'.format(impact_score_path,str(lca_iteration_name))\n",
    "                    write_LCA_obj_to_HDF5_file(impact_score,hdf5_file_MC_LCA_results,impact_score_path)\n",
    "                    \n",
    "                    #Uncertainty LCI 1 LCIA 0\n",
    "                    impact_score_lcia0= characterization_matrix_lcia0 * inventory\n",
    "                    impact_score_lcia0_path='{}/{}'.format(impact_score_lcia0_path,str(lca_iteration_name))\n",
    "                    write_LCA_obj_to_HDF5_file(impact_score_lcia0,hdf5_file_MC_LCA_results,impact_score_lcia0_path)\n",
    "                    \n",
    "                    #Uncertainty LCI 0 LCIA 1\n",
    "                    impact_score_lci0_lcia1= characterization_matrix * inventory_lci0\n",
    "                    impact_score_lci0_lcia1_path='{}/{}'.format(impact_score_lci0_lcia1_path,str(lca_iteration_name))\n",
    "                    write_LCA_obj_to_HDF5_file(impact_score_lci0_lcia1,hdf5_file_MC_LCA_results,impact_score_lci0_lcia1_path)\n",
    "                    \n",
    "                    #store info on activities and EF?\n",
    "                    \n",
    "                #For aggregated results --> impact_score is a scalar, store as vector for each impact method    \n",
    "                else:\n",
    "                    #Uncertainty LCI 1 LCIA 1\n",
    "                    characterization_matrix=np.reshape(characterization_matrix,characterization_matrix.shape[0])\n",
    "                    impact_score= np.dot(characterization_matrix, inventory)\n",
    "                    append_to_list_hdf5_dataset(hdf5_file_MC_LCA_results,impact_score_path,np.atleast_1d(impact_score))\n",
    "                    \n",
    "                    #Uncertainty LCI 1 LCIA 0\n",
    "                    characterization_matrix_lcia0=np.array(characterization_matrix_lcia0.sum(1))\n",
    "                    characterization_matrix_lcia0= np.reshape(characterization_matrix_lcia0, characterization_matrix_lcia0.shape[0])\n",
    "                    impact_score_lcia0= np.dot(characterization_matrix_lcia0, inventory)\n",
    "                    append_to_list_hdf5_dataset(hdf5_file_MC_LCA_results, impact_score_lcia0_path, np.atleast_1d(impact_score_lcia0))\n",
    "                    \n",
    "                    #Uncertainty LCI 0 LCIA 1\n",
    "                    impact_score_lci0_lcia1= np.dot(characterization_matrix, inventory_lci0)\n",
    "                    append_to_list_hdf5_dataset(hdf5_file_MC_LCA_results, impact_score_lci0_lcia1_path, np.atleast_1d(impact_score_lci0_lcia1))\n",
    "                    \n",
    "                #end_4 = time.time()\n",
    "                #print(\"Work per impact method/activity/iteration en {} secondes for iteration {} for activity {} for impact method {}\".format(end_4 - start_4, iteration, actKey, impact_method_name))    \n",
    "            \n",
    "                        \n",
    "            \n",
    "        #Store the list of iteration names from LCI\n",
    "        lci_iteration_name_list_path='/lci_iteration_name_list'\n",
    "        append_to_list_hdf5_dataset(hdf5_file_MC_LCA_results,lci_iteration_name_list_path,np.atleast_1d(lci_iteration_name))\n",
    "        \n",
    "        \n",
    "        #Count the number of complete iterations for LCA results\n",
    "        lca_complete_iterations=lca_iteration_name+1\n",
    "        hdf5_file_MC_LCA_results.attrs['Number of complete iterations']= lca_complete_iterations\n",
    "        \n",
    "        end_iteration = time.time()\n",
    "        print(\"Work per iteration en {} secondes for iteration {} for worker {} \".format(end_iteration - start_iteration, iteration,worker_id))\n",
    "                                            \n",
    "            \n",
    "                        \n",
    "    #Size of stored objects\n",
    "    #if results_disaggregated_or_not == \"disaggregated\":\n",
    "        #group_path_techno=impact_score_path\n",
    "        #hdf5_file_MC_results=hdf5_file_MC_LCA_results\n",
    "        #size_results=(hdf5_file_MC_results[group_path_techno+'/data'].id.get_storage_size()+hdf5_file_MC_results[group_path_techno+'/indptr'].id.get_storage_size()+hdf5_file_MC_results[group_path_techno+'/indices'].id.get_storage_size())/1000000\n",
    "        #print(\"Size of stored disaggregated results = {} MB for one impact method/activity/iteration \".format(size_results))\n",
    "    #else: \n",
    "        #size_results=hdf5_file_MC_LCA_results[impact_score_path].id.get_storage_size()/1000000\n",
    "        #print(\"Size of stored aggregated results = {} MB for {} complete iterations for one impact method/activity \".format(size_results, lca_complete_iterations))\n",
    "\n",
    "    \n",
    "    hdf5_file_deterministic_lci.close()\n",
    "    hdf5_file_MC_LCI_results.close()\n",
    "    hdf5_file_MC_LCA_results.close()\n",
    "    \n",
    "    return;\n",
    "                       \n",
    "                        \n",
    "                \n",
    "                \n",
    "        \n",
    "def get_deterministic_inventory(collector_functional_unit, functional_units, hdf5_file_deterministic_lci, job_id,results_disaggregated_or_not):\n",
    "    \n",
    "    start_1 = time.time()\n",
    "    \n",
    "    #Get metadata for the HDF5 file\n",
    "    DB_name= list(functional_units[0].keys())[0][0]\n",
    "    \n",
    "    #Write useful information to HDF5 file\n",
    "    hdf5_file_deterministic_lci.attrs['Database name']=DB_name\n",
    "    hdf5_file_deterministic_lci.attrs['Creation ID']=job_id\n",
    "    hdf5_file_deterministic_lci.attrs['Description']='HDF5 file containing the deterministic values for inventory for all activities in the database'\n",
    "    \n",
    "    #Retrieve B matrix for uncertainty LCI 0 --> use sacrificial LCA\n",
    "    sacrificial_lca = LCA(collector_functional_unit)\n",
    "    sacrificial_lca.lci()\n",
    "    biosphere_matrix_lci0=sacrificial_lca.biosphere_matrix\n",
    "    count = len(sacrificial_lca.activity_dict)\n",
    "    \n",
    "    \n",
    "    #Retrieve the supply_array for uncertainty LCI 0  \n",
    "    for act_index, fu in enumerate(functional_units):\n",
    "        \n",
    "        #Creating UUID for each activity\n",
    "        actKey = list(fu.keys())[0][1]\n",
    "        \n",
    "        #Calculate the deterministic supply_array\n",
    "        lca = LCA(fu)\n",
    "        lca.lci()\n",
    "        supply_array_lci0=lca.supply_array\n",
    "        \n",
    "        #Calculate deterministic inventories. inventory_lci0_disaggregated is a csr matrix.and Store deterministic inventories\n",
    "        if results_disaggregated_or_not == \"disaggregated\":\n",
    "            inventory_lci0_disaggregated = biosphere_matrix_lci0 * sparse.spdiags([supply_array_lci0], [0], count, count)\n",
    "            inventory_lci0_disaggregated_path='/inventory/{}'.format(actKey)\n",
    "            write_LCA_obj_to_HDF5_file(inventory_lci0_disaggregated,hdf5_file_deterministic_lci,inventory_lci0_disaggregated_path)\n",
    "        else:\n",
    "            inventory_lci0_aggregated = biosphere_matrix_lci0 * supply_array_lci0\n",
    "            inventory_lci0_aggregated_path='/inventory/{}'.format(actKey)\n",
    "            write_LCA_obj_to_HDF5_file(inventory_lci0_aggregated,hdf5_file_deterministic_lci,inventory_lci0_aggregated_path)\n",
    "            \n",
    "    end_1 = time.time()\n",
    "    \n",
    "    print('Deterministic LCI calculated in {} sec'.format(end_1-start_1))\n",
    "\n",
    "    return;    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Dependant_LCA_Monte_Carlo_results(project, \n",
    "                                                 database, \n",
    "                                                 iterations, \n",
    "                                                 cpus,\n",
    "                                      activity_category_ID,\n",
    "                                                 hdf5_file_MC_LCI_results_path, \n",
    "                                                 path_for_saving,\n",
    "                                                 impact_method_name,\n",
    "                                                 results_disaggregated_or_not):\n",
    "    \n",
    "    projects.set_current(project)\n",
    "    bw2setup()\n",
    "    \n",
    "    impact_method_name_list=[ic_name for ic_name in methods if impact_method_name in str(ic_name)]\n",
    "\n",
    "    #Path the write the results\n",
    "    BASE_OUTPUT_DIR = path_for_saving\n",
    "\n",
    "    #ID to identify who and when was the calculation made\n",
    "    now = datetime.datetime.now()\n",
    "    COMPUTERNAME=os.environ['COMPUTERNAME'] #for Windows\n",
    "    #COMPUTERNAME='computer_name' #for linux\n",
    "    job_id = \"{}_{}-{}-{}_{}h{}\".format(COMPUTERNAME,now.year, now.month, now.day, now.hour, now.minute)\n",
    "\n",
    "    #Selection of activities for MC analysis\n",
    "    db = Database(database)\n",
    "    activities = [activity for activity in db if activity_category_ID in str(activity['classifications'])]\n",
    "    #act1=db.get('e929619f245df590fee5d72dc979cdd4')\n",
    "    #act2=db.get('bdf7116059abfcc6b8b9ade1a641e578')\n",
    "    #act3=db.get('c8c815c68836adaf964daaa001a638a3')\n",
    "    #activities = [act1,act2,act3]\n",
    "    \n",
    "    #Create objects to pass the functional units = 1 for each activity\n",
    "    functional_units = [ {act.key: 1} for act in activities ]\n",
    "    collector_functional_unit = {k:v for d in functional_units for k, v in d.items()}\n",
    "    \n",
    "    #Create and save all the useful information related to the deterministic version of the database\n",
    "    path_for_saving=BASE_OUTPUT_DIR\n",
    "    hdf5_file_name_agg=\"Deterministic_aggregated_LCI.hdf5\"\n",
    "    hdf5_file_name_disagg=\"Deterministic_disaggregated_LCI.hdf5\"\n",
    "    hdf5_file_deterministic_lci_path_agg=os.path.join(path_for_saving,hdf5_file_name_agg)\n",
    "    hdf5_file_deterministic_lci_path_disagg=os.path.join(path_for_saving,hdf5_file_name_disagg)\n",
    "    \n",
    "    if results_disaggregated_or_not == \"disaggregated\":\n",
    "        hdf5_file_deterministic_lci_path=hdf5_file_deterministic_lci_path_disagg\n",
    "    else:\n",
    "        hdf5_file_deterministic_lci_path=hdf5_file_deterministic_lci_path_agg\n",
    "    \n",
    "    if os.path.isfile(hdf5_file_deterministic_lci_path)==False:\n",
    "        hdf5_file_deterministic_lci=h5py.File(hdf5_file_deterministic_lci_path,'a')\n",
    "        get_deterministic_inventory(collector_functional_unit, functional_units, hdf5_file_deterministic_lci, job_id,results_disaggregated_or_not)\n",
    "        hdf5_file_deterministic_lci.close()\n",
    "    \n",
    "    #Code to slipt the work between each CPUs of the computer (called workers). The work refers here to the dependant LCI MC for each activity \n",
    "    workers = []\n",
    "\n",
    "    for worker_id in range(cpus):\n",
    "        #Create or open the HDF5 file for each worker and write metadata\n",
    "        hdf5_file_name=\"LCA_Dependant_Monte_Carlo_aggregated_results_worker{}.hdf5\".format(str(worker_id))\n",
    "        hdf5_file_MC_LCA_results_aggregated_path=os.path.join(BASE_OUTPUT_DIR,hdf5_file_name)\n",
    "        hdf5_file_name=\"LCA_Dependant_Monte_Carlo_disaggregated_results_worker{}.hdf5\".format(str(worker_id))\n",
    "        hdf5_file_MC_LCA_results_disaggregated_path=os.path.join(BASE_OUTPUT_DIR,hdf5_file_name)\n",
    "        \n",
    "        if results_disaggregated_or_not == \"disaggregated\":\n",
    "            hdf5_file_MC_results=h5py.File(hdf5_file_MC_LCA_results_disaggregated_path,'a')\n",
    "        else:\n",
    "            hdf5_file_MC_results=h5py.File(hdf5_file_MC_LCA_results_aggregated_path,'a')\n",
    "        \n",
    "        hdf5_file_MC_results.attrs['Database name']=db.name\n",
    "        hdf5_file_MC_results.attrs['Worker ID']=worker_id\n",
    "        hdf5_file_MC_results.attrs['Description']='HDF5 file containing all LCA dependant Monte Carlo results per activity/iteration'\n",
    "\n",
    "        hdf5_file_MC_results.close()\n",
    "\n",
    "        # Create child processes that can work apart from parent process\n",
    "        child = mp.Process(target=worker_process, args=(projects.current, \n",
    "                                                        job_id, worker_id, \n",
    "                                                        iterations,\n",
    "                                                        functional_units,\n",
    "                                                        collector_functional_unit,\n",
    "                                                        hdf5_file_MC_LCI_results_path,\n",
    "                                                        hdf5_file_deterministic_lci_path_agg,\n",
    "                                                        hdf5_file_deterministic_lci_path_disagg,\n",
    "                                                        hdf5_file_MC_LCA_results_aggregated_path,\n",
    "                                                        hdf5_file_MC_LCA_results_disaggregated_path,\n",
    "                                                        impact_method_name_list,\n",
    "                                                        results_disaggregated_or_not))\n",
    "        workers.append(child)\n",
    "        child.start()\n",
    "        \n",
    "    return;\n",
    "         \n",
    "        \n",
    "#Useful when the code is run from the console to execute the main function\n",
    "if __name__ == '__main__':\n",
    "    Dependant_LCA_Monte_Carlo_results(project=\"iw_integration\", \n",
    "                                      database=\"ecoinvent 3.3 cutoff\", \n",
    "                                      iterations=1250, \n",
    "                                      cpus=4,\n",
    "                                      activity_category_ID=\"4922\",\n",
    "                                      hdf5_file_MC_LCI_results_path=\"D:\\\\Dropbox (MAGI)\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Monte Carlo results_with correlation\\\\Dependant LCI Monte Carlo - sector 4922\\\\LCI_Dependant_Monte_Carlo_results_ALL.hdf5\", \n",
    "                                      path_for_saving=\"D:\\\\Dropbox (MAGI)\\\\Dossiers professionnels\\\\Logiciels\\\\Brightway 2\\\\Monte Carlo results_with correlation\\\\Dependant LCA Monte Carlo - sector 4922\",\n",
    "                                      impact_method_name='IMPACTWorld+ (Default_Recommended_Endpoint 1_36)',\n",
    "                                      results_disaggregated_or_not=\"aggregated\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
